{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#purpose","title":"Purpose","text":"<p>Lemmatization is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. Unlike stemming, lemmatization outputs word units that are still valid linguistic forms.</p> <p>In modern natural language processing (NLP), this task is often indirectly tackled by more complex systems encompassing a whole processing pipeline. However, it appears that there is no straightforward way to address lemmatization in Python although this task can be crucial in fields such as information retrieval and NLP.</p> <p>Simplemma provides a simple and multilingual approach to look for base forms or lemmata. It may not be as powerful as full-fledged solutions but it is generic, easy to install and straightforward to use. In particular, it does not need morphosyntactic information and can process a raw series of tokens or even a text with its built-in tokenizer. By design it should be reasonably fast and work in a large majority of cases, without being perfect.</p> <p>With its comparatively small footprint it is especially useful when speed and simplicity matter, in low-resource contexts, for educational purposes, or as a baseline system for lemmatization and morphological analysis.</p> <p>Currently, 48 languages are partly or fully supported (see table below).</p>"},{"location":"#installation","title":"Installation","text":"<p>The current library is written in pure Python with no dependencies:</p> <p><code>pip install simplemma</code></p> <ul> <li><code>pip3</code> where applicable</li> <li><code>pip install -U simplemma</code> for updates</li> </ul>"},{"location":"#usage","title":"Usage","text":""},{"location":"#word-by-word","title":"Word-by-word","text":"<p>Simplemma is used by selecting a language of interest and then applying the data on a list of words.</p> <pre><code>    &gt;&gt;&gt; import simplemma\n    # get a word\n    myword = 'masks'\n    # decide which language to use and apply it on a word form\n    &gt;&gt;&gt; simplemma.lemmatize(myword, lang='en')\n    'mask'\n    # grab a list of tokens\n    &gt;&gt;&gt; mytokens = ['Hier', 'sind', 'Vaccines']\n    &gt;&gt;&gt; for token in mytokens:\n    &gt;&gt;&gt;     simplemma.lemmatize(token, lang='de')\n    'hier'\n    'sein'\n    'Vaccines'\n    # list comprehensions can be faster\n    &gt;&gt;&gt; [simplemma.lemmatize(t, lang='de') for t in mytokens]\n    ['hier', 'sein', 'Vaccines']\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributions","title":"Contributions","text":""},{"location":"contributing/#issues-bugs-improvment-ideas","title":"Issues, bugs &amp; improvment ideas","text":"<p>Feel free to contribute, notably by filing issues for feedback, bug reports, or links to further lemmatization lists, rules and tests.</p> <p>You can also contribute to this lemmatization list repository.</p>"},{"location":"contributing/#contributing-to-the-code","title":"Contributing to the code","text":"<p>Pull requests are welcome.</p> <p>Before creating the pull request, please ensure that it pass all the quality checks:</p> <pre><code># Install dev dependencies\npip install -r requirements.dev.txt\n# Check code style\nblack --check --diff simplemma training tests\n# Check typings\nmypy -p simplemma -p training -p tests\n# Run tests\npytest --cov=./ --cov-report=xml\n\n</code></pre> <p>Also, if your PR changes needs documentation changes, see the next section.</p>"},{"location":"contributing/#contributing-to-documentation","title":"Contributing to documentation","text":"<p>```sh</p>"},{"location":"contributing/#install-dev-dependencies","title":"Install dev dependencies","text":"<p>pip install -r requirements.docs.txt</p>"},{"location":"contributing/#build-mkdocs","title":"Build mkdocs","text":"<p>mkdocs build</p>"},{"location":"contributing/#check-typings","title":"Check typings","text":"<p>mypy -p simplemma -p training -p tests</p>"},{"location":"contributing/#run-tests","title":"Run tests","text":"<p>pytest --cov=./ --cov-report=xml</p>"},{"location":"credit-and-licenses/","title":"Credits and licenses","text":"<p>Software under MIT license, for the linguistic information databases see <code>licenses</code> folder.</p> <p>The surface lookups (non-greedy mode) use lemmatization lists derived from various sources, ordered by relative importance:</p> <ul> <li>Lemmatization lists by Michal M\u011bchura (Open Database License)</li> <li>Wiktionary entries packaged by the Kaikki project</li> <li>FreeLing project</li> <li>spaCy lookups data</li> <li>Unimorph Project</li> <li>Wikinflection corpus by Eleni Metheniti (CC BY 4.0 License)</li> </ul>"},{"location":"supported-languages/","title":"Supported languages","text":"<p>The following languages are available using their ISO 639-1 code:</p>"},{"location":"supported-languages/#available-languages-2022-09-05","title":"Available languages (2022-09-05)","text":"Code Language Forms (10\u00b3) Acc. Comments <code>bg</code> Bulgarian 213 <code>ca</code> Catalan 579 <code>cs</code> Czech 187 0.88 on UD CS-PDT <code>cy</code> Welsh 360 <code>da</code> Danish 554 0.92 on UD DA-DDT, alternative: lemmy <code>de</code> German 682 0.95 on UD DE-GSD, see also German-NLP list <code>el</code> Greek 183 0.88 on UD EL-GDT <code>en</code> English 136 0.94 on UD EN-GUM, alternative: LemmInflect <code>enm</code> Middle English 38 <code>es</code> Spanish 720 0.94 on UD ES-GSD <code>et</code> Estonian 133 low coverage <code>fa</code> Persian 10 experimental <code>fi</code> Finnish 2,106 evaluation and alternatives: see this benchmark <code>fr</code> French 217 0.94 on UD FR-GSD <code>ga</code> Irish 383 <code>gd</code> Gaelic 48 <code>gl</code> Galician 384 <code>gv</code> Manx 62 <code>hbs</code> Serbo-Croatian 838 Croatian and Serbian lists to be added later <code>hi</code> Hindi 58 experimental <code>hu</code> Hungarian 458 <code>hy</code> Armenian 323 <code>id</code> Indonesian 17 0.91 on UD ID-CSUI <code>is</code> Icelandic 175 <code>it</code> Italian 333 0.93 on UD IT-ISDT <code>ka</code> Georgian 65 <code>la</code> Latin 850 <code>lb</code> Luxembourgish 305 <code>lt</code> Lithuanian 247 <code>lv</code> Latvian 168 <code>mk</code> Macedonian 57 <code>ms</code> Malay 14 <code>nb</code> Norwegian (Bokm\u00e5l) 617 <code>nl</code> Dutch 254 0.91 on UD-NL-Alpino <code>nn</code> Norwegian (Nynorsk) <code>pl</code> Polish 3,733 0.91 on UD-PL-PDB <code>pt</code> Portuguese 933 0.92 on UD-PT-GSD <code>ro</code> Romanian 311 <code>ru</code> Russian 607 alternative: pymorphy2 <code>se</code> Northern S\u00e1mi 113 experimental <code>sk</code> Slovak 846 0.92 on UD SK-SNK <code>sl</code> Slovene 136 <code>sq</code> Albanian 35 <code>sv</code> Swedish 658 alternative: lemmy <code>sw</code> Swahili 10 experimental <code>tl</code> Tagalog 33 experimental <code>tr</code> Turkish 1,333 0.88 on UD-TR-Boun <code>uk</code> Ukrainian 190 alternative: pymorphy2 <p>Low coverage mentions means one would probably be better off with a language-specific library, but simplemma will work to a limited extent. Open-source alternatives for Python are referenced if possible.</p> <p>Experimental mentions indicate that the language remains untested or that there could be issues with the underlying data or lemmatization process.</p> <p>The scores are calculated on Universal Dependencies treebanks on single word tokens (including some contractions but not merged prepositions), they describe to what extent simplemma can accurately map tokens to their lemma form. They can be reproduced by concatenating all available UD files and by using the script <code>udscore.py</code> in the <code>training/</code> folder.</p> <p>This library is particularly relevant as regards the lemmatization of less frequent words. Its performance in this case is only incidentally captured by the benchmark above. In some languages, a fixed number of words such as pronouns can be further mapped by hand to enhance performance.</p>"},{"location":"reference/language_detector/","title":"Language Detector","text":"<p>Lemmatizer module. Provides classes for text language detection using lemmatization and token sampling.</p> <ul> <li>LanguageDetector: Class for performing language detection using lemmatization and token sampling.</li> <li>in_target_language(): A legacy function that wraps the LanguageDetector's is_known() method.</li> <li>langdetect(): A legacy function that wraps the LanguageDetector's is_known() method.</li> </ul>"},{"location":"reference/language_detector/#simplemma.language_detector-classes","title":"Classes","text":""},{"location":"reference/language_detector/#simplemma.language_detector.LanguageDetector","title":"<code>LanguageDetector</code>","text":"<p>A class that performs language detection using lemmatization and token sampling.</p> Source code in <code>simplemma/language_detector.py</code> <pre><code>class LanguageDetector:\n    \"\"\"A class that performs language detection using lemmatization and token sampling.\"\"\"\n\n    __slots__ = [\n        \"_lang\",\n        \"_lemmatization_strategy\",\n        \"_orig_token_sampler\",\n        \"_token_sampler\",\n    ]\n\n    def __init__(\n        self,\n        lang: Union[str, Tuple[str, ...]],\n        token_sampler: TokenSampler = MostCommonTokenSampler(),\n        lemmatization_strategy: LemmatizationStrategy = DefaultStrategy(),\n    ) -&gt; None:\n        \"\"\"\n        Initialize the LanguageDetector.\n\n        Args:\n            lang (Union[str, Tuple[str, ...]]): The target language or languages to detect.\n            token_sampler (TokenSampler, optional): The token sampling strategy to use.\n                Defaults to `MostCommonTokenSampler()`.\n            lemmatization_strategy (LemmatizationStrategy, optional): The lemmatization\n                strategy to use. `Defaults to DefaultStrategy()`.\n        \"\"\"\n\n        self._lang = validate_lang_input(lang)\n        self._token_sampler = token_sampler\n        self._orig_token_sampler = token_sampler\n        self._lemmatization_strategy = lemmatization_strategy\n\n    def _restore_token_sampler(self) -&gt; None:\n        self._token_sampler = self._orig_token_sampler\n\n    def proportion_in_each_language(\n        self,\n        text: str,\n    ) -&gt; Dict[str, float]:\n        \"\"\"\n        Calculate the proportion of each language in the given text.\n\n        Args:\n            text (str): The input text to analyze.\n\n        Returns:\n            Dict[str, float]: A dictionary containing the detected languages and\n                their respective proportions.\n        \"\"\"\n        tokens = self._token_sampler.sample_text(text)\n\n        total_tokens = len(tokens)\n        if total_tokens == 0:\n            return {\"unk\": 1}\n\n        known_tokens_count = dict.fromkeys(self._lang, 0)\n        unknown_tokens_count = 0\n        for token in tokens:\n            token_found = False\n            for lang_code in self._lang:\n                candidate = self._lemmatization_strategy.get_lemma(token, lang_code)\n                if candidate is not None:\n                    known_tokens_count[lang_code] += 1\n                    token_found = True\n            if not token_found:\n                unknown_tokens_count += 1\n\n        results: Dict[str, float] = dict(\n            (lang_code, token_count / total_tokens)\n            for (lang_code, token_count) in known_tokens_count.items()\n        )\n        results[\"unk\"] = unknown_tokens_count / total_tokens\n        return results\n\n    def proportion_in_target_languages(\n        self,\n        text: str,\n    ) -&gt; float:\n        \"\"\"\n        Calculate the proportion of text in the target language.\n\n        Args:\n            text (str): The input text to analyze.\n\n        Returns:\n            float: The proportion of text in the target language(s).\n        \"\"\"\n        tokens = self._token_sampler.sample_text(text)\n        if len(tokens) == 0:\n            return 0\n\n        in_target = 0\n        for token in tokens:\n            for lang_code in self._lang:\n                candidate = self._lemmatization_strategy.get_lemma(token, lang_code)\n                if candidate is not None:\n                    in_target += 1\n                    break\n        return in_target / len(tokens)\n\n    def main_language(\n        self,\n        text: str,\n        additional_token_samplers: List[TokenSampler] = [\n            RelaxedMostCommonTokenSampler()\n        ],\n    ) -&gt; str:\n        \"\"\"\n        Determine the main language of the given text.\n\n        Args:\n            text (str): The input text to analyze.\n            additional_token_samplers (List[TokenSampler], optional): Additional token\n                sampling strategies to use. Defaults to `[RelaxedMostCommonTokenSampler()]`.\n\n        Returns:\n            str: The main language of the text.\n        \"\"\"\n        token_samplers = [self._token_sampler] + additional_token_samplers\n\n        for token_sampler in token_samplers:\n            self._token_sampler = token_sampler\n            list_results = _as_list(self.proportion_in_each_language(text))\n            if len(list_results) &gt; 1 and list_results[0][1] != list_results[1][1]:\n                self._restore_token_sampler()\n                return list_results[0][0]\n\n        self._restore_token_sampler()\n        return \"unk\"\n</code></pre>"},{"location":"reference/language_detector/#simplemma.language_detector.LanguageDetector-functions","title":"Functions","text":""},{"location":"reference/language_detector/#simplemma.language_detector.LanguageDetector.__init__","title":"<code>__init__(lang, token_sampler=MostCommonTokenSampler(), lemmatization_strategy=DefaultStrategy())</code>","text":"<p>Initialize the LanguageDetector.</p> <p>Parameters:</p> Name Type Description Default <code>lang</code> <code>Union[str, Tuple[str, ...]]</code> <p>The target language or languages to detect.</p> required <code>token_sampler</code> <code>TokenSampler</code> <p>The token sampling strategy to use. Defaults to <code>MostCommonTokenSampler()</code>.</p> <code>MostCommonTokenSampler()</code> <code>lemmatization_strategy</code> <code>LemmatizationStrategy</code> <p>The lemmatization strategy to use. <code>Defaults to DefaultStrategy()</code>.</p> <code>DefaultStrategy()</code> Source code in <code>simplemma/language_detector.py</code> <pre><code>def __init__(\n    self,\n    lang: Union[str, Tuple[str, ...]],\n    token_sampler: TokenSampler = MostCommonTokenSampler(),\n    lemmatization_strategy: LemmatizationStrategy = DefaultStrategy(),\n) -&gt; None:\n    \"\"\"\n    Initialize the LanguageDetector.\n\n    Args:\n        lang (Union[str, Tuple[str, ...]]): The target language or languages to detect.\n        token_sampler (TokenSampler, optional): The token sampling strategy to use.\n            Defaults to `MostCommonTokenSampler()`.\n        lemmatization_strategy (LemmatizationStrategy, optional): The lemmatization\n            strategy to use. `Defaults to DefaultStrategy()`.\n    \"\"\"\n\n    self._lang = validate_lang_input(lang)\n    self._token_sampler = token_sampler\n    self._orig_token_sampler = token_sampler\n    self._lemmatization_strategy = lemmatization_strategy\n</code></pre>"},{"location":"reference/language_detector/#simplemma.language_detector.LanguageDetector.main_language","title":"<code>main_language(text, additional_token_samplers=[RelaxedMostCommonTokenSampler()])</code>","text":"<p>Determine the main language of the given text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to analyze.</p> required <code>additional_token_samplers</code> <code>List[TokenSampler]</code> <p>Additional token sampling strategies to use. Defaults to <code>[RelaxedMostCommonTokenSampler()]</code>.</p> <code>[RelaxedMostCommonTokenSampler()]</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The main language of the text.</p> Source code in <code>simplemma/language_detector.py</code> <pre><code>def main_language(\n    self,\n    text: str,\n    additional_token_samplers: List[TokenSampler] = [\n        RelaxedMostCommonTokenSampler()\n    ],\n) -&gt; str:\n    \"\"\"\n    Determine the main language of the given text.\n\n    Args:\n        text (str): The input text to analyze.\n        additional_token_samplers (List[TokenSampler], optional): Additional token\n            sampling strategies to use. Defaults to `[RelaxedMostCommonTokenSampler()]`.\n\n    Returns:\n        str: The main language of the text.\n    \"\"\"\n    token_samplers = [self._token_sampler] + additional_token_samplers\n\n    for token_sampler in token_samplers:\n        self._token_sampler = token_sampler\n        list_results = _as_list(self.proportion_in_each_language(text))\n        if len(list_results) &gt; 1 and list_results[0][1] != list_results[1][1]:\n            self._restore_token_sampler()\n            return list_results[0][0]\n\n    self._restore_token_sampler()\n    return \"unk\"\n</code></pre>"},{"location":"reference/language_detector/#simplemma.language_detector.LanguageDetector.proportion_in_each_language","title":"<code>proportion_in_each_language(text)</code>","text":"<p>Calculate the proportion of each language in the given text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to analyze.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: A dictionary containing the detected languages and their respective proportions.</p> Source code in <code>simplemma/language_detector.py</code> <pre><code>def proportion_in_each_language(\n    self,\n    text: str,\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Calculate the proportion of each language in the given text.\n\n    Args:\n        text (str): The input text to analyze.\n\n    Returns:\n        Dict[str, float]: A dictionary containing the detected languages and\n            their respective proportions.\n    \"\"\"\n    tokens = self._token_sampler.sample_text(text)\n\n    total_tokens = len(tokens)\n    if total_tokens == 0:\n        return {\"unk\": 1}\n\n    known_tokens_count = dict.fromkeys(self._lang, 0)\n    unknown_tokens_count = 0\n    for token in tokens:\n        token_found = False\n        for lang_code in self._lang:\n            candidate = self._lemmatization_strategy.get_lemma(token, lang_code)\n            if candidate is not None:\n                known_tokens_count[lang_code] += 1\n                token_found = True\n        if not token_found:\n            unknown_tokens_count += 1\n\n    results: Dict[str, float] = dict(\n        (lang_code, token_count / total_tokens)\n        for (lang_code, token_count) in known_tokens_count.items()\n    )\n    results[\"unk\"] = unknown_tokens_count / total_tokens\n    return results\n</code></pre>"},{"location":"reference/language_detector/#simplemma.language_detector.LanguageDetector.proportion_in_target_languages","title":"<code>proportion_in_target_languages(text)</code>","text":"<p>Calculate the proportion of text in the target language.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to analyze.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The proportion of text in the target language(s).</p> Source code in <code>simplemma/language_detector.py</code> <pre><code>def proportion_in_target_languages(\n    self,\n    text: str,\n) -&gt; float:\n    \"\"\"\n    Calculate the proportion of text in the target language.\n\n    Args:\n        text (str): The input text to analyze.\n\n    Returns:\n        float: The proportion of text in the target language(s).\n    \"\"\"\n    tokens = self._token_sampler.sample_text(text)\n    if len(tokens) == 0:\n        return 0\n\n    in_target = 0\n    for token in tokens:\n        for lang_code in self._lang:\n            candidate = self._lemmatization_strategy.get_lemma(token, lang_code)\n            if candidate is not None:\n                in_target += 1\n                break\n    return in_target / len(tokens)\n</code></pre>"},{"location":"reference/language_detector/#simplemma.language_detector-functions","title":"Functions","text":""},{"location":"reference/language_detector/#simplemma.language_detector.in_target_language","title":"<code>in_target_language(text, lang, greedy=False, token_sampler=MostCommonTokenSampler())</code>","text":"<p>Calculate the proportion of text in the target language(s).</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to analyze.</p> required <code>lang</code> <code>Union[str, Tuple[str, ...]]</code> <p>The target language(s) to compare against.</p> required <code>greedy</code> <code>bool</code> <p>Whether to use greedy lemmatization. Defaults to <code>False</code>.</p> <code>False</code> <code>token_sampler</code> <code>TokenSampler</code> <p>The token sampling strategy to use. Defaults to <code>MostCommonTokenSampler()</code>.</p> <code>MostCommonTokenSampler()</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The proportion of text in the target language(s).</p> Source code in <code>simplemma/language_detector.py</code> <pre><code>def in_target_language(\n    text: str,\n    lang: Union[str, Tuple[str, ...]],\n    greedy: bool = False,\n    token_sampler: TokenSampler = MostCommonTokenSampler(),\n) -&gt; float:\n    \"\"\"\n    Calculate the proportion of text in the target language(s).\n\n    Args:\n        text (str): The input text to analyze.\n        lang (Union[str, Tuple[str, ...]]): The target language(s) to compare against.\n        greedy (bool, optional): Whether to use greedy lemmatization. Defaults to `False`.\n        token_sampler (TokenSampler, optional): The token sampling strategy to use.\n            Defaults to `MostCommonTokenSampler()`.\n\n    Returns:\n        float: The proportion of text in the target language(s).\n    \"\"\"\n\n    return LanguageDetector(\n        lang, token_sampler, DefaultStrategy(greedy)\n    ).proportion_in_target_languages(text)\n</code></pre>"},{"location":"reference/language_detector/#simplemma.language_detector.langdetect","title":"<code>langdetect(text, lang, greedy=False, token_samplers=[MostCommonTokenSampler(), RelaxedMostCommonTokenSampler()])</code>","text":"<p>Detect the language(s) of the given text and their proportions.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to analyze.</p> required <code>lang</code> <code>Union[str, Tuple[str, ...]]</code> <p>The target language(s) to compare against.</p> required <code>greedy</code> <code>bool</code> <p>Whether to use greedy lemmatization. Defaults to <code>False</code>.</p> <code>False</code> <code>token_samplers</code> <code>List[TokenSampler]</code> <p>The list of token sampling strategies to use. Defaults to <code>[MostCommonTokenSampler(), RelaxedMostCommonTokenSampler()]</code>.</p> <code>[MostCommonTokenSampler(), RelaxedMostCommonTokenSampler()]</code> <p>Returns:</p> Type Description <code>List[Tuple[str, float]]</code> <p>List[Tuple[str, float]]: A list of tuples containing the detected language(s) and their respective proportions.</p> Source code in <code>simplemma/language_detector.py</code> <pre><code>def langdetect(\n    text: str,\n    lang: Union[str, Tuple[str, ...]],\n    greedy: bool = False,\n    token_samplers: List[TokenSampler] = [\n        MostCommonTokenSampler(),\n        RelaxedMostCommonTokenSampler(),\n    ],\n) -&gt; List[Tuple[str, float]]:\n    \"\"\"\n    Detect the language(s) of the given text and their proportions.\n\n    Args:\n        text (str): The input text to analyze.\n        lang (Union[str, Tuple[str, ...]]): The target language(s) to compare against.\n        greedy (bool, optional): Whether to use greedy lemmatization. Defaults to `False`.\n        token_samplers (List[TokenSampler], optional): The list of token sampling strategies\n            to use. Defaults to `[MostCommonTokenSampler(), RelaxedMostCommonTokenSampler()]`.\n\n    Returns:\n        List[Tuple[str, float]]: A list of tuples containing the detected language(s)\n            and their respective proportions.\n    \"\"\"\n\n    for token_sampler in token_samplers:\n        results = LanguageDetector(\n            lang, token_sampler, DefaultStrategy(greedy)\n        ).proportion_in_each_language(text)\n\n        # post-processing\n        list_results = _as_list(results)\n        if len(list_results) == 1 or list_results[0][1] != list_results[1][1]:\n            return list_results\n    return list_results\n</code></pre>"},{"location":"reference/lemmatizer/","title":"Lemmatizer","text":"<p>Lemmatizer module. Provides classes for lemmatizing token and full texts.</p> <ul> <li>Lemmatizer: Class for performing token and full text lemmatization.</li> <li>is_known(): A legacy function that wraps the Lemmatizer's is_known() method.</li> <li>lemmatize(): A legacy function that wraps the Lemmatizer's lemmatize() method.</li> <li>text_lemmatizer(): A legacy function that wraps the Lemmatizer's text_lemmatizer() method.</li> <li>lemma_iterator(): A legacy function that wraps the Lemmatizer's lemma_iterator() method.</li> </ul>"},{"location":"reference/lemmatizer/#simplemma.lemmatizer-classes","title":"Classes","text":""},{"location":"reference/lemmatizer/#simplemma.lemmatizer.Lemmatizer","title":"<code>Lemmatizer</code>","text":"<p>Lemmatizer class for performing token lemmatization.</p> Source code in <code>simplemma/lemmatizer.py</code> <pre><code>class Lemmatizer:\n    \"\"\"Lemmatizer class for performing token lemmatization.\"\"\"\n\n    __slots__ = [\n        \"_cached_lemmatize\",\n        \"_fallback_lemmatization_strategy\",\n        \"_lemmatization_strategy\",\n        \"_tokenizer\",\n    ]\n\n    def __init__(\n        self,\n        cache_max_size: int = 1048576,\n        tokenizer: Tokenizer = RegexTokenizer(),\n        lemmatization_strategy: LemmatizationStrategy = DefaultStrategy(),\n        fallback_lemmatization_strategy: LemmatizationFallbackStrategy = ToLowercaseFallbackStrategy(),\n    ) -&gt; None:\n        \"\"\"\n        Initialize the Lemmatizer.\n\n        Args:\n            cache_max_size (int, optional): The maximum size of the cache for the lemmatization results.\n                Defaults to `1048576`.\n            tokenizer (Tokenizer, optional): The tokenizer to use for tokenization.\n                Defaults to `RegexTokenizer()`.\n            lemmatization_strategy (LemmatizationStrategy, optional): The lemmatization strategy to use.\n                Defaults to `DefaultStrategy()`.\n            fallback_lemmatization_strategy (LemmatizationFallbackStrategy, optional): The fallback lemmatization strategy to use.\n                Defaults to `ToLowercaseFallbackStrategy()`.\n\n        \"\"\"\n        self._tokenizer = tokenizer\n        self._lemmatization_strategy = lemmatization_strategy\n        self._fallback_lemmatization_strategy = fallback_lemmatization_strategy\n        self._cached_lemmatize = lru_cache(maxsize=cache_max_size)(self._lemmatize)\n\n    def lemmatize(\n        self,\n        token: str,\n        lang: Union[str, Tuple[str, ...]],\n    ) -&gt; str:\n        \"\"\"Get the lemmatized form of a given word in the specified language(s).\n\n        Args:\n            token: The token to lemmatize.\n            lang: The language or languages for lemmatization.\n\n        Returns:\n            str: The lemmatized form of the token.\n        \"\"\"\n        return self._cached_lemmatize(token, lang)\n\n    def _lemmatize(\n        self,\n        token: str,\n        lang: Union[str, Tuple[str, ...]],\n    ) -&gt; str:\n        \"\"\"Internal method to lemmatize a token in the specified language(s).\n\n        Args:\n            token: The token to lemmatize.\n            lang: The language or languages for lemmatization.\n\n        Returns:\n            str: The lemmatized form of the token.\n        \"\"\"\n        _control_input_type(token)\n        lang = validate_lang_input(lang)\n\n        for lang_code in lang:\n            candidate = self._lemmatization_strategy.get_lemma(token, lang_code)\n            if candidate is not None:\n                return candidate\n\n        return self._fallback_lemmatization_strategy.get_lemma(token, next(iter(lang)))\n\n    def get_lemmas_in_text(\n        self,\n        text: str,\n        lang: Union[str, Tuple[str, ...]],\n    ) -&gt; Iterator[str]:\n        \"\"\"Get an iterator over lemmatized tokens in a text.\n\n        Args:\n            text: The text to process.\n            lang: The language or languages for lemmatization.\n\n        Yields:\n            str: The lemmatized tokens in the text.\n        \"\"\"\n        initial = True\n        for token in self._tokenizer.split_text(text):\n            yield self.lemmatize(token.lower() if initial else token, lang)\n            initial = token in PUNCTUATION\n</code></pre>"},{"location":"reference/lemmatizer/#simplemma.lemmatizer.Lemmatizer-functions","title":"Functions","text":""},{"location":"reference/lemmatizer/#simplemma.lemmatizer.Lemmatizer.__init__","title":"<code>__init__(cache_max_size=1048576, tokenizer=RegexTokenizer(), lemmatization_strategy=DefaultStrategy(), fallback_lemmatization_strategy=ToLowercaseFallbackStrategy())</code>","text":"<p>Initialize the Lemmatizer.</p> <p>Parameters:</p> Name Type Description Default <code>cache_max_size</code> <code>int</code> <p>The maximum size of the cache for the lemmatization results. Defaults to <code>1048576</code>.</p> <code>1048576</code> <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use for tokenization. Defaults to <code>RegexTokenizer()</code>.</p> <code>RegexTokenizer()</code> <code>lemmatization_strategy</code> <code>LemmatizationStrategy</code> <p>The lemmatization strategy to use. Defaults to <code>DefaultStrategy()</code>.</p> <code>DefaultStrategy()</code> <code>fallback_lemmatization_strategy</code> <code>LemmatizationFallbackStrategy</code> <p>The fallback lemmatization strategy to use. Defaults to <code>ToLowercaseFallbackStrategy()</code>.</p> <code>ToLowercaseFallbackStrategy()</code> Source code in <code>simplemma/lemmatizer.py</code> <pre><code>def __init__(\n    self,\n    cache_max_size: int = 1048576,\n    tokenizer: Tokenizer = RegexTokenizer(),\n    lemmatization_strategy: LemmatizationStrategy = DefaultStrategy(),\n    fallback_lemmatization_strategy: LemmatizationFallbackStrategy = ToLowercaseFallbackStrategy(),\n) -&gt; None:\n    \"\"\"\n    Initialize the Lemmatizer.\n\n    Args:\n        cache_max_size (int, optional): The maximum size of the cache for the lemmatization results.\n            Defaults to `1048576`.\n        tokenizer (Tokenizer, optional): The tokenizer to use for tokenization.\n            Defaults to `RegexTokenizer()`.\n        lemmatization_strategy (LemmatizationStrategy, optional): The lemmatization strategy to use.\n            Defaults to `DefaultStrategy()`.\n        fallback_lemmatization_strategy (LemmatizationFallbackStrategy, optional): The fallback lemmatization strategy to use.\n            Defaults to `ToLowercaseFallbackStrategy()`.\n\n    \"\"\"\n    self._tokenizer = tokenizer\n    self._lemmatization_strategy = lemmatization_strategy\n    self._fallback_lemmatization_strategy = fallback_lemmatization_strategy\n    self._cached_lemmatize = lru_cache(maxsize=cache_max_size)(self._lemmatize)\n</code></pre>"},{"location":"reference/lemmatizer/#simplemma.lemmatizer.Lemmatizer.get_lemmas_in_text","title":"<code>get_lemmas_in_text(text, lang)</code>","text":"<p>Get an iterator over lemmatized tokens in a text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to process.</p> required <code>lang</code> <code>Union[str, Tuple[str, ...]]</code> <p>The language or languages for lemmatization.</p> required <p>Yields:</p> Name Type Description <code>str</code> <code>str</code> <p>The lemmatized tokens in the text.</p> Source code in <code>simplemma/lemmatizer.py</code> <pre><code>def get_lemmas_in_text(\n    self,\n    text: str,\n    lang: Union[str, Tuple[str, ...]],\n) -&gt; Iterator[str]:\n    \"\"\"Get an iterator over lemmatized tokens in a text.\n\n    Args:\n        text: The text to process.\n        lang: The language or languages for lemmatization.\n\n    Yields:\n        str: The lemmatized tokens in the text.\n    \"\"\"\n    initial = True\n    for token in self._tokenizer.split_text(text):\n        yield self.lemmatize(token.lower() if initial else token, lang)\n        initial = token in PUNCTUATION\n</code></pre>"},{"location":"reference/lemmatizer/#simplemma.lemmatizer.Lemmatizer.lemmatize","title":"<code>lemmatize(token, lang)</code>","text":"<p>Get the lemmatized form of a given word in the specified language(s).</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>str</code> <p>The token to lemmatize.</p> required <code>lang</code> <code>Union[str, Tuple[str, ...]]</code> <p>The language or languages for lemmatization.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The lemmatized form of the token.</p> Source code in <code>simplemma/lemmatizer.py</code> <pre><code>def lemmatize(\n    self,\n    token: str,\n    lang: Union[str, Tuple[str, ...]],\n) -&gt; str:\n    \"\"\"Get the lemmatized form of a given word in the specified language(s).\n\n    Args:\n        token: The token to lemmatize.\n        lang: The language or languages for lemmatization.\n\n    Returns:\n        str: The lemmatized form of the token.\n    \"\"\"\n    return self._cached_lemmatize(token, lang)\n</code></pre>"},{"location":"reference/lemmatizer/#simplemma.lemmatizer-functions","title":"Functions","text":""},{"location":"reference/lemmatizer/#simplemma.lemmatizer.is_known","title":"<code>is_known(token, lang)</code>","text":"<p>Check if a token is known in the specified language(s).</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>str</code> <p>The token to check.</p> required <code>lang</code> <code>Union[str, Tuple[str, ...]]</code> <p>The language or languages to check in.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the token is known, False otherwise.</p> Source code in <code>simplemma/lemmatizer.py</code> <pre><code>def is_known(token: str, lang: Union[str, Tuple[str, ...]]) -&gt; bool:\n    \"\"\"Check if a token is known in the specified language(s).\n\n    Args:\n        token: The token to check.\n        lang: The language or languages to check in.\n\n    Returns:\n        bool: True if the token is known, False otherwise.\n    \"\"\"\n\n    _control_input_type(token)\n    lang = validate_lang_input(lang)\n\n    dictionary_lookup = DictionaryLookupStrategy(_legacy_dictionary_factory)\n    return any(\n        dictionary_lookup.get_lemma(token, lang_code) is not None for lang_code in lang\n    )\n</code></pre>"},{"location":"reference/lemmatizer/#simplemma.lemmatizer.lemma_iterator","title":"<code>lemma_iterator(text, lang, greedy=False)</code>","text":"<p>Iterate over lemmatized tokens in a text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to iterate over.</p> required <code>lang</code> <code>Union[str, Tuple[str, ...]]</code> <p>The language or languages for lemmatization.</p> required <code>greedy</code> <code>bool</code> <p>A flag indicating whether to use greedy lemmatization (default: False).</p> <code>False</code> <code>tokenizer</code> <p>The tokenizer to use (default: RegexTokenizer()).</p> required <p>Yields:</p> Name Type Description <code>str</code> <code>str</code> <p>The lemmatized tokens in the text.</p> Source code in <code>simplemma/lemmatizer.py</code> <pre><code>def lemma_iterator(\n    text: str, lang: Union[str, Tuple[str, ...]], greedy: bool = False\n) -&gt; Iterator[str]:\n    \"\"\"Iterate over lemmatized tokens in a text.\n\n    Args:\n        text: The text to iterate over.\n        lang: The language or languages for lemmatization.\n        greedy: A flag indicating whether to use greedy lemmatization (default: False).\n        tokenizer: The tokenizer to use (default: RegexTokenizer()).\n\n    Yields:\n        str: The lemmatized tokens in the text.\n    \"\"\"\n    lemmatizer = _legacy_lemmatizer if not greedy else _legacy_greedy_lemmatizer\n    return lemmatizer.get_lemmas_in_text(text, lang)\n</code></pre>"},{"location":"reference/lemmatizer/#simplemma.lemmatizer.lemmatize","title":"<code>lemmatize(token, lang, greedy=False)</code>","text":"<p>Lemmatize a token in the specified language(s).</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>str</code> <p>The token to lemmatize.</p> required <code>lang</code> <code>Union[str, Tuple[str, ...]]</code> <p>The language or languages for lemmatization.</p> required <code>greedy</code> <code>bool</code> <p>A flag indicating whether to use greedy lemmatization (default: False).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The lemmatized form of the token.</p> Source code in <code>simplemma/lemmatizer.py</code> <pre><code>def lemmatize(\n    token: str, lang: Union[str, Tuple[str, ...]], greedy: bool = False\n) -&gt; str:\n    \"\"\"Lemmatize a token in the specified language(s).\n\n    Args:\n        token: The token to lemmatize.\n        lang: The language or languages for lemmatization.\n        greedy: A flag indicating whether to use greedy lemmatization (default: False).\n\n    Returns:\n        str: The lemmatized form of the token.\n    \"\"\"\n    lemmatizer = _legacy_lemmatizer if not greedy else _legacy_greedy_lemmatizer\n    return lemmatizer.lemmatize(token, lang)\n</code></pre>"},{"location":"reference/lemmatizer/#simplemma.lemmatizer.text_lemmatizer","title":"<code>text_lemmatizer(text, lang, greedy=False)</code>","text":"<p>Lemmatize a text in the specified language(s).</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to lemmatize.</p> required <code>lang</code> <code>Union[str, Tuple[str, ...]]</code> <p>The language or languages for lemmatization.</p> required <code>greedy</code> <code>bool</code> <p>A flag indicating whether to use greedy lemmatization (default: False).</p> <code>False</code> <code>tokenizer</code> <p>The tokenizer to use (default: RegexTokenizer()).</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The list of lemmatized tokens.</p> Source code in <code>simplemma/lemmatizer.py</code> <pre><code>def text_lemmatizer(\n    text: str, lang: Union[str, Tuple[str, ...]], greedy: bool = False\n) -&gt; List[str]:\n    \"\"\"Lemmatize a text in the specified language(s).\n\n    Args:\n        text: The text to lemmatize.\n        lang: The language or languages for lemmatization.\n        greedy: A flag indicating whether to use greedy lemmatization (default: False).\n        tokenizer: The tokenizer to use (default: RegexTokenizer()).\n\n    Returns:\n        List[str]: The list of lemmatized tokens.\n    \"\"\"\n\n    return list(\n        lemma_iterator(\n            text,\n            lang,\n            greedy,\n        )\n    )\n</code></pre>"},{"location":"reference/token_sampler/","title":"TokenSampler","text":"<p>Token Sampler module. Provides classes for sampling tokens from text.</p> <ul> <li>TokenSampler: The Protocol class for all token samplers.</li> <li>BaseTokenSampler: An abstract base class for token samplers implementing tokenization using a Tokenizer so the user only has to implement the sampling strategy.</li> <li>MostCommonTokenSampler: A token sampler that selects the most common tokens.</li> <li>RelaxedMostCommonTokenSampler: A relaxed version of the most common token sampler.</li> </ul>"},{"location":"reference/token_sampler/#simplemma.token_sampler-classes","title":"Classes","text":""},{"location":"reference/token_sampler/#simplemma.token_sampler.BaseTokenSampler","title":"<code>BaseTokenSampler</code>","text":"<p>             Bases: <code>ABC</code>, <code>TokenSampler</code></p> <p>BaseTokenSampler is the base class for token samplers. It uses the given Tokenizer to convert a text in token. Classes inheriting from BaseTokenSampler only have to implement sample_tokens.</p> Source code in <code>simplemma/token_sampler.py</code> <pre><code>class BaseTokenSampler(ABC, TokenSampler):\n    \"\"\"\n    BaseTokenSampler is the base class for token samplers.\n    It uses the given Tokenizer to convert a text in token.\n    Classes inheriting from BaseTokenSampler only have to implement sample_tokens.\n    \"\"\"\n\n    __slots__ = [\"_tokenizer\"]\n\n    def __init__(\n        self,\n        tokenizer: Tokenizer = RegexTokenizer(SPLIT_INPUT),\n    ) -&gt; None:\n        \"\"\"\n        Initialize the BaseTokenSampler.\n\n        Args:\n            tokenizer (Tokenizer, optional): The tokenizer to use for splitting text into tokens.\n                Defaults to `RegexTokenizer(SPLIT_INPUT)`.\n        \"\"\"\n        self._tokenizer = tokenizer\n\n    def sample_text(self, text: str) -&gt; List[str]:\n        \"\"\"\n        Sample tokens from the input text.\n\n        Args:\n            text (str): The input text to sample tokens from.\n\n        Returns:\n            List[str]: The sampled tokens.\n\n        \"\"\"\n        return self.sample_tokens(self._tokenizer.split_text(text))\n\n    @abstractmethod\n    def sample_tokens(self, tokens: Iterable[str]) -&gt; List[str]:\n        \"\"\"\n        Sample tokens from the given iterable of tokens.\n\n        Args:\n            tokens (Iterable[str]): The iterable of tokens to sample from.\n\n        Returns:\n            List[str]: The sampled tokens.\n\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/token_sampler/#simplemma.token_sampler.BaseTokenSampler-functions","title":"Functions","text":""},{"location":"reference/token_sampler/#simplemma.token_sampler.BaseTokenSampler.__init__","title":"<code>__init__(tokenizer=RegexTokenizer(SPLIT_INPUT))</code>","text":"<p>Initialize the BaseTokenSampler.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use for splitting text into tokens. Defaults to <code>RegexTokenizer(SPLIT_INPUT)</code>.</p> <code>RegexTokenizer(SPLIT_INPUT)</code> Source code in <code>simplemma/token_sampler.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: Tokenizer = RegexTokenizer(SPLIT_INPUT),\n) -&gt; None:\n    \"\"\"\n    Initialize the BaseTokenSampler.\n\n    Args:\n        tokenizer (Tokenizer, optional): The tokenizer to use for splitting text into tokens.\n            Defaults to `RegexTokenizer(SPLIT_INPUT)`.\n    \"\"\"\n    self._tokenizer = tokenizer\n</code></pre>"},{"location":"reference/token_sampler/#simplemma.token_sampler.BaseTokenSampler.sample_text","title":"<code>sample_text(text)</code>","text":"<p>Sample tokens from the input text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to sample tokens from.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The sampled tokens.</p> Source code in <code>simplemma/token_sampler.py</code> <pre><code>def sample_text(self, text: str) -&gt; List[str]:\n    \"\"\"\n    Sample tokens from the input text.\n\n    Args:\n        text (str): The input text to sample tokens from.\n\n    Returns:\n        List[str]: The sampled tokens.\n\n    \"\"\"\n    return self.sample_tokens(self._tokenizer.split_text(text))\n</code></pre>"},{"location":"reference/token_sampler/#simplemma.token_sampler.BaseTokenSampler.sample_tokens","title":"<code>sample_tokens(tokens)</code>  <code>abstractmethod</code>","text":"<p>Sample tokens from the given iterable of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>Iterable[str]</code> <p>The iterable of tokens to sample from.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The sampled tokens.</p> Source code in <code>simplemma/token_sampler.py</code> <pre><code>@abstractmethod\ndef sample_tokens(self, tokens: Iterable[str]) -&gt; List[str]:\n    \"\"\"\n    Sample tokens from the given iterable of tokens.\n\n    Args:\n        tokens (Iterable[str]): The iterable of tokens to sample from.\n\n    Returns:\n        List[str]: The sampled tokens.\n\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/token_sampler/#simplemma.token_sampler.MostCommonTokenSampler","title":"<code>MostCommonTokenSampler</code>","text":"<p>             Bases: <code>BaseTokenSampler</code></p> <p>Token sampler that selects the most common tokens.</p> Source code in <code>simplemma/token_sampler.py</code> <pre><code>class MostCommonTokenSampler(BaseTokenSampler):\n    \"\"\"Token sampler that selects the most common tokens.\"\"\"\n\n    __slots__ = [\"_capitalized_threshold\", \"_sample_size\"]\n\n    def __init__(\n        self,\n        tokenizer: Tokenizer = RegexTokenizer(SPLIT_INPUT),\n        sample_size: int = 100,\n        capitalized_threshold: float = 0.8,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the MostCommonTokenSampler.\n\n        Args:\n            tokenizer (Tokenizer, optional): The tokenizer to use for splitting text into tokens.\n                Defaults to `RegexTokenizer(SPLIT_INPUT)`.\n            sample_size (int, optional): The number of tokens to sample. Defaults to `100`.\n            capitalized_threshold (float, optional): The threshold for removing capitalized tokens.\n                Tokens with a frequency greater than this threshold will be removed. Defaults to `0.8`.\n        \"\"\"\n        super().__init__(tokenizer)\n        self._sample_size = sample_size\n        self._capitalized_threshold = capitalized_threshold\n\n    def sample_tokens(self, tokens: Iterable[str]) -&gt; List[str]:\n        \"\"\"\n        Sample tokens from the given iterable of tokens.\n\n        Args:\n            tokens (Iterable[str]): The iterable of tokens to sample from.\n\n        Returns:\n            List[str]: The sampled tokens.\n\n        \"\"\"\n        counter = Counter(tokens)\n\n        if self._capitalized_threshold &gt; 0:\n            deletions = [token for token in counter if token[0].isupper()]\n            if len(deletions) &lt; self._capitalized_threshold * len(counter):\n                for token in deletions:\n                    del counter[token]\n\n        return [item[0] for item in counter.most_common(self._sample_size)]\n</code></pre>"},{"location":"reference/token_sampler/#simplemma.token_sampler.MostCommonTokenSampler-functions","title":"Functions","text":""},{"location":"reference/token_sampler/#simplemma.token_sampler.MostCommonTokenSampler.__init__","title":"<code>__init__(tokenizer=RegexTokenizer(SPLIT_INPUT), sample_size=100, capitalized_threshold=0.8)</code>","text":"<p>Initialize the MostCommonTokenSampler.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use for splitting text into tokens. Defaults to <code>RegexTokenizer(SPLIT_INPUT)</code>.</p> <code>RegexTokenizer(SPLIT_INPUT)</code> <code>sample_size</code> <code>int</code> <p>The number of tokens to sample. Defaults to <code>100</code>.</p> <code>100</code> <code>capitalized_threshold</code> <code>float</code> <p>The threshold for removing capitalized tokens. Tokens with a frequency greater than this threshold will be removed. Defaults to <code>0.8</code>.</p> <code>0.8</code> Source code in <code>simplemma/token_sampler.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: Tokenizer = RegexTokenizer(SPLIT_INPUT),\n    sample_size: int = 100,\n    capitalized_threshold: float = 0.8,\n) -&gt; None:\n    \"\"\"\n    Initialize the MostCommonTokenSampler.\n\n    Args:\n        tokenizer (Tokenizer, optional): The tokenizer to use for splitting text into tokens.\n            Defaults to `RegexTokenizer(SPLIT_INPUT)`.\n        sample_size (int, optional): The number of tokens to sample. Defaults to `100`.\n        capitalized_threshold (float, optional): The threshold for removing capitalized tokens.\n            Tokens with a frequency greater than this threshold will be removed. Defaults to `0.8`.\n    \"\"\"\n    super().__init__(tokenizer)\n    self._sample_size = sample_size\n    self._capitalized_threshold = capitalized_threshold\n</code></pre>"},{"location":"reference/token_sampler/#simplemma.token_sampler.MostCommonTokenSampler.sample_tokens","title":"<code>sample_tokens(tokens)</code>","text":"<p>Sample tokens from the given iterable of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>Iterable[str]</code> <p>The iterable of tokens to sample from.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The sampled tokens.</p> Source code in <code>simplemma/token_sampler.py</code> <pre><code>def sample_tokens(self, tokens: Iterable[str]) -&gt; List[str]:\n    \"\"\"\n    Sample tokens from the given iterable of tokens.\n\n    Args:\n        tokens (Iterable[str]): The iterable of tokens to sample from.\n\n    Returns:\n        List[str]: The sampled tokens.\n\n    \"\"\"\n    counter = Counter(tokens)\n\n    if self._capitalized_threshold &gt; 0:\n        deletions = [token for token in counter if token[0].isupper()]\n        if len(deletions) &lt; self._capitalized_threshold * len(counter):\n            for token in deletions:\n                del counter[token]\n\n    return [item[0] for item in counter.most_common(self._sample_size)]\n</code></pre>"},{"location":"reference/token_sampler/#simplemma.token_sampler.RelaxedMostCommonTokenSampler","title":"<code>RelaxedMostCommonTokenSampler</code>","text":"<p>             Bases: <code>MostCommonTokenSampler</code></p> <p>Relaxed version of the most common token sampler. This sampler uses a relaxed splitting regex pattern and allows for a larger sample size.</p> Source code in <code>simplemma/token_sampler.py</code> <pre><code>class RelaxedMostCommonTokenSampler(MostCommonTokenSampler):\n    \"\"\"\n    Relaxed version of the most common token sampler.\n    This sampler uses a relaxed splitting regex pattern and allows for a larger sample size.\n    \"\"\"\n\n    def __init__(\n        self,\n        tokenizer: Tokenizer = RegexTokenizer(RELAXED_SPLIT_INPUT),\n        sample_size: int = 1000,\n        capitalized_threshold: float = 0,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the RelaxedMostCommonTokenSampler.\n        This is just a `MostCommonTokenSampler` with a more relaxed regex pattern.\n\n        Args:\n            tokenizer (Tokenizer, optional): The tokenizer to use for splitting text into tokens.\n                Defaults to `RegexTokenizer(RELAXED_SPLIT_INPUT)`.\n            sample_size (int, optional): The number of tokens to sample. Defaults to `1000`.\n            capitalized_threshold (float, optional): The threshold for removing capitalized tokens.\n                Tokens with a frequency greater than this threshold will be removed.\n                Defaults to `0`.\n\n        \"\"\"\n\n        super().__init__(tokenizer, sample_size, capitalized_threshold)\n</code></pre>"},{"location":"reference/token_sampler/#simplemma.token_sampler.RelaxedMostCommonTokenSampler-functions","title":"Functions","text":""},{"location":"reference/token_sampler/#simplemma.token_sampler.RelaxedMostCommonTokenSampler.__init__","title":"<code>__init__(tokenizer=RegexTokenizer(RELAXED_SPLIT_INPUT), sample_size=1000, capitalized_threshold=0)</code>","text":"<p>Initialize the RelaxedMostCommonTokenSampler. This is just a <code>MostCommonTokenSampler</code> with a more relaxed regex pattern.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use for splitting text into tokens. Defaults to <code>RegexTokenizer(RELAXED_SPLIT_INPUT)</code>.</p> <code>RegexTokenizer(RELAXED_SPLIT_INPUT)</code> <code>sample_size</code> <code>int</code> <p>The number of tokens to sample. Defaults to <code>1000</code>.</p> <code>1000</code> <code>capitalized_threshold</code> <code>float</code> <p>The threshold for removing capitalized tokens. Tokens with a frequency greater than this threshold will be removed. Defaults to <code>0</code>.</p> <code>0</code> Source code in <code>simplemma/token_sampler.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: Tokenizer = RegexTokenizer(RELAXED_SPLIT_INPUT),\n    sample_size: int = 1000,\n    capitalized_threshold: float = 0,\n) -&gt; None:\n    \"\"\"\n    Initialize the RelaxedMostCommonTokenSampler.\n    This is just a `MostCommonTokenSampler` with a more relaxed regex pattern.\n\n    Args:\n        tokenizer (Tokenizer, optional): The tokenizer to use for splitting text into tokens.\n            Defaults to `RegexTokenizer(RELAXED_SPLIT_INPUT)`.\n        sample_size (int, optional): The number of tokens to sample. Defaults to `1000`.\n        capitalized_threshold (float, optional): The threshold for removing capitalized tokens.\n            Tokens with a frequency greater than this threshold will be removed.\n            Defaults to `0`.\n\n    \"\"\"\n\n    super().__init__(tokenizer, sample_size, capitalized_threshold)\n</code></pre>"},{"location":"reference/token_sampler/#simplemma.token_sampler.TokenSampler","title":"<code>TokenSampler</code>","text":"<p>             Bases: <code>Protocol</code></p> <p>Abstract base class for token samplers.</p> <p>Token samplers are used to sample tokens from text.</p> Source code in <code>simplemma/token_sampler.py</code> <pre><code>class TokenSampler(Protocol):\n    \"\"\"\n    Abstract base class for token samplers.\n\n    Token samplers are used to sample tokens from text.\n\n    \"\"\"\n\n    @abstractmethod\n    def sample_text(self, text: str) -&gt; List[str]:\n        \"\"\"\n        Sample tokens from the input text.\n\n        Args:\n            text (str): The input text to sample tokens from.\n\n        Returns:\n            List[str]: The sampled tokens.\n\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def sample_tokens(self, tokens: Iterable[str]) -&gt; List[str]:\n        \"\"\"\n        Sample tokens from the given iterable of tokens.\n\n        Args:\n            tokens (Iterable[str]): The iterable of tokens to sample from.\n\n        Returns:\n            List[str]: The sampled tokens.\n\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/token_sampler/#simplemma.token_sampler.TokenSampler-functions","title":"Functions","text":""},{"location":"reference/token_sampler/#simplemma.token_sampler.TokenSampler.sample_text","title":"<code>sample_text(text)</code>  <code>abstractmethod</code>","text":"<p>Sample tokens from the input text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to sample tokens from.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The sampled tokens.</p> Source code in <code>simplemma/token_sampler.py</code> <pre><code>@abstractmethod\ndef sample_text(self, text: str) -&gt; List[str]:\n    \"\"\"\n    Sample tokens from the input text.\n\n    Args:\n        text (str): The input text to sample tokens from.\n\n    Returns:\n        List[str]: The sampled tokens.\n\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/token_sampler/#simplemma.token_sampler.TokenSampler.sample_tokens","title":"<code>sample_tokens(tokens)</code>  <code>abstractmethod</code>","text":"<p>Sample tokens from the given iterable of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>Iterable[str]</code> <p>The iterable of tokens to sample from.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The sampled tokens.</p> Source code in <code>simplemma/token_sampler.py</code> <pre><code>@abstractmethod\ndef sample_tokens(self, tokens: Iterable[str]) -&gt; List[str]:\n    \"\"\"\n    Sample tokens from the given iterable of tokens.\n\n    Args:\n        tokens (Iterable[str]): The iterable of tokens to sample from.\n\n    Returns:\n        List[str]: The sampled tokens.\n\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/tokenizer/","title":"Tokenizer","text":"<p>Tokenizers module. Provides classes for text tokenization.</p> <ul> <li>Tokenizer: The Protocol class for all tokenizers.</li> <li>RegexTokenizer: A tokenizer based on a regular expresion.</li> <li>simple_tokenizer(): A legacy function that wraps the RegexTokenizer's split_text method.</li> <li>TOKREGEX: The regular expresion used by default by RegexTokenizer.</li> </ul>"},{"location":"reference/tokenizer/#simplemma.tokenizer-attributes","title":"Attributes","text":""},{"location":"reference/tokenizer/#simplemma.tokenizer.TOKREGEX","title":"<code>TOKREGEX = re.compile('(?:(?:[\u20ac$\uffe5\u00a3+-]?[0-9][0-9.,:%/-]*|St\\\\.)[\\\\w_\u20ac-]+|https?://[^ ]+|[\u20ac$\uffe5\u00a3@#\u00a7]?\\\\w[\\\\w*_-]*|[,;:\\\\.?!\u00bf\u00a1\u203d\u2e2e\u2026()\\\\[\\\\]\u2013{}\u2014\u2015/\u2012_\u201c\u201e\u201d\u2e42\u201a\u2018\u2019\u201b\u2032\u2033\u201f\\'\\\\\"\u00ab\u00bb\u2039\u203a&lt;&gt;=+\u2212\u00d7\u00f7\u2022\u00b7]+)')</code>  <code>module-attribute</code>","text":"<p>The regular expresion used by default by RegexTokenizer.</p>"},{"location":"reference/tokenizer/#simplemma.tokenizer-classes","title":"Classes","text":""},{"location":"reference/tokenizer/#simplemma.tokenizer.RegexTokenizer","title":"<code>RegexTokenizer</code>","text":"<p>             Bases: <code>Tokenizer</code></p> <p>Tokenizer that uses regular expressions to split a text into tokens. This tokenizer splits the input text using the specified regex pattern.</p> Source code in <code>simplemma/tokenizer.py</code> <pre><code>class RegexTokenizer(Tokenizer):\n    \"\"\"\n    Tokenizer that uses regular expressions to split a text into tokens.\n    This tokenizer splits the input text using the specified regex pattern.\n    \"\"\"\n\n    __slots__ = [\"_splitting_regex\"]\n\n    def __init__(self, splitting_regex: Pattern[str] = TOKREGEX) -&gt; None:\n        self._splitting_regex = splitting_regex\n\n    def split_text(self, text: str) -&gt; Iterator[str]:\n        \"\"\"\n        Split the input text using the specified regex pattern.\n\n        Args:\n            text (str): The input text to tokenize.\n\n        Returns:\n            Iterator[str]: An iterator yielding the individual tokens.\n\n        \"\"\"\n        return (match[0] for match in self._splitting_regex.finditer(text))\n</code></pre>"},{"location":"reference/tokenizer/#simplemma.tokenizer.RegexTokenizer-functions","title":"Functions","text":""},{"location":"reference/tokenizer/#simplemma.tokenizer.RegexTokenizer.split_text","title":"<code>split_text(text)</code>","text":"<p>Split the input text using the specified regex pattern.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to tokenize.</p> required <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>Iterator[str]: An iterator yielding the individual tokens.</p> Source code in <code>simplemma/tokenizer.py</code> <pre><code>def split_text(self, text: str) -&gt; Iterator[str]:\n    \"\"\"\n    Split the input text using the specified regex pattern.\n\n    Args:\n        text (str): The input text to tokenize.\n\n    Returns:\n        Iterator[str]: An iterator yielding the individual tokens.\n\n    \"\"\"\n    return (match[0] for match in self._splitting_regex.finditer(text))\n</code></pre>"},{"location":"reference/tokenizer/#simplemma.tokenizer.Tokenizer","title":"<code>Tokenizer</code>","text":"<p>             Bases: <code>Protocol</code></p> <p>Abstract base class for Tokenizers. Tokenizers are used to split a text into individual tokens.</p> Source code in <code>simplemma/tokenizer.py</code> <pre><code>class Tokenizer(Protocol):\n    \"\"\"\n    Abstract base class for Tokenizers.\n    Tokenizers are used to split a text into individual tokens.\n    \"\"\"\n\n    @abstractmethod\n    def split_text(self, text: str) -&gt; Iterator[str]:\n        \"\"\"\n        Split the input text into tokens.\n\n        Args:\n            text (str): The input text to tokenize.\n\n        Returns:\n            Iterator[str]: An iterator yielding the individual tokens.\n\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/tokenizer/#simplemma.tokenizer.Tokenizer-functions","title":"Functions","text":""},{"location":"reference/tokenizer/#simplemma.tokenizer.Tokenizer.split_text","title":"<code>split_text(text)</code>  <code>abstractmethod</code>","text":"<p>Split the input text into tokens.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to tokenize.</p> required <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>Iterator[str]: An iterator yielding the individual tokens.</p> Source code in <code>simplemma/tokenizer.py</code> <pre><code>@abstractmethod\ndef split_text(self, text: str) -&gt; Iterator[str]:\n    \"\"\"\n    Split the input text into tokens.\n\n    Args:\n        text (str): The input text to tokenize.\n\n    Returns:\n        Iterator[str]: An iterator yielding the individual tokens.\n\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/tokenizer/#simplemma.tokenizer-functions","title":"Functions","text":""},{"location":"reference/tokenizer/#simplemma.tokenizer.simple_tokenizer","title":"<code>simple_tokenizer(text)</code>","text":"<p>Simple regular expression tokenizer.</p> <p>This function takes a string as input and returns a list of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to tokenize.</p> required <code>splitting_regex</code> <code>Pattern[str]</code> <p>The regular expression pattern used for tokenization. Defaults to <code>TOKREGEX</code>.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The list of tokens extracted from the input text.</p> Source code in <code>simplemma/tokenizer.py</code> <pre><code>def simple_tokenizer(text: str) -&gt; List[str]:\n    \"\"\"\n    Simple regular expression tokenizer.\n\n    This function takes a string as input and returns a list of tokens.\n\n    Args:\n        text (str): The input text to tokenize.\n        splitting_regex (Pattern[str], optional): The regular expression pattern used for tokenization.\n            Defaults to `TOKREGEX`.\n\n    Returns:\n        List[str]: The list of tokens extracted from the input text.\n\n    \"\"\"\n    return list(_legacy_tokenizer.split_text(text))\n</code></pre>"},{"location":"reference/utils/","title":"Utils","text":"<p>Utils module. Contains utility functions for language processing.</p> <ul> <li>levenshtein_dist: Calculates the Levenshtein distance between two strings.</li> <li>validate_lang_input: Validates the language input and ensures it is a valid tuple.</li> </ul>"},{"location":"reference/utils/#simplemma.utils-functions","title":"Functions","text":""},{"location":"reference/utils/#simplemma.utils.levenshtein_dist","title":"<code>levenshtein_dist(str1, str2)</code>","text":"<p>Calculate the Levenshtein distance between two strings.</p> <p>The Levenshtein distance is a metric for measuring the difference between two strings, defined as the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other.</p> <p>Parameters:</p> Name Type Description Default <code>str1</code> <code>str</code> <p>The first string.</p> required <code>str2</code> <code>str</code> <p>The second string.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The Levenshtein distance between the two strings.</p> Source code in <code>simplemma/utils.py</code> <pre><code>def levenshtein_dist(str1: str, str2: str) -&gt; int:\n    \"\"\"\n    Calculate the Levenshtein distance between two strings.\n\n    The Levenshtein distance is a metric for measuring the difference between two strings,\n    defined as the minimum number of single-character edits (insertions, deletions, or substitutions)\n    required to change one string into the other.\n\n    Args:\n        str1 (str): The first string.\n        str2 (str): The second string.\n\n    Returns:\n        int: The Levenshtein distance between the two strings.\n\n    \"\"\"\n    # inspired by this noticeably faster code:\n    # https://gist.github.com/p-hash/9e0f9904ce7947c133308fbe48fe032b\n    if str1 == str2:\n        return 0\n    if len(str1) &gt; len(str2):\n        str1, str2 = str2, str1\n    r1 = list(range(len(str2) + 1))\n    r2 = [0] * len(r1)\n    for i, c1 in enumerate(str1):\n        r2[0] = i + 1\n        for j, c2 in enumerate(str2):\n            if c1 == c2:\n                r2[j + 1] = r1[j]\n            else:\n                a1, a2, a3 = r2[j], r1[j], r1[j + 1]\n                if a1 &gt; a2:\n                    if a2 &gt; a3:\n                        r2[j + 1] = 1 + a3\n                    else:\n                        r2[j + 1] = 1 + a2\n                else:\n                    if a1 &gt; a3:\n                        r2[j + 1] = 1 + a3\n                    else:\n                        r2[j + 1] = 1 + a1\n        aux = r1\n        r1, r2 = r2, aux\n    return r1[-1]\n</code></pre>"},{"location":"reference/utils/#simplemma.utils.validate_lang_input","title":"<code>validate_lang_input(lang)</code>","text":"<p>Make sure the lang variable is a valid tuple.</p> <p>Parameters:</p> Name Type Description Default <code>lang</code> <code>Any</code> <p>The language input.</p> required <p>Returns:</p> Type Description <code>Tuple[str]</code> <p>Tuple[str]: A tuple containing the language code.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the lang argument is not a tuple or a string.</p> Source code in <code>simplemma/utils.py</code> <pre><code>def validate_lang_input(lang: Union[str, Tuple[str, ...]]) -&gt; Tuple[str]:\n    \"\"\"\n    Make sure the lang variable is a valid tuple.\n\n    Args:\n        lang (Any): The language input.\n\n    Returns:\n        Tuple[str]: A tuple containing the language code.\n\n    Raises:\n        TypeError: If the lang argument is not a tuple or a string.\n\n    \"\"\"\n    # convert string\n    if isinstance(lang, str):\n        lang = (lang,)\n    if not isinstance(lang, tuple):\n        raise TypeError(\"lang argument must be a two-letter language code\")\n    return lang  # type: ignore[return-value]\n</code></pre>"},{"location":"reference/strategies/affix_decomposition/","title":"Affix Decomposition Strategy","text":"<p>This file defines the <code>AffixDecompositionStrategy</code> class, which implements an affix decomposition lemmatization strategy in the Simplemma library.</p>"},{"location":"reference/strategies/affix_decomposition/#simplemma.strategies.affix_decomposition-classes","title":"Classes","text":""},{"location":"reference/strategies/affix_decomposition/#simplemma.strategies.affix_decomposition.AffixDecompositionStrategy","title":"<code>AffixDecompositionStrategy</code>","text":"<p>             Bases: <code>LemmatizationStrategy</code></p> <p>Lemmatization strategy that uses affix decomposition to find lemmas of tokens.</p> <p>This strategy decomposes tokens into affixes and looks up their lemmas in a dictionary. It first attempts to decompose the token using affix decomposition and then falls back to suffix decomposition if affix decomposition fails.</p> Source code in <code>simplemma/strategies/affix_decomposition.py</code> <pre><code>class AffixDecompositionStrategy(LemmatizationStrategy):\n    \"\"\"\n    Lemmatization strategy that uses affix decomposition to find lemmas of tokens.\n\n    This strategy decomposes tokens into affixes and looks up their lemmas in a dictionary.\n    It first attempts to decompose the token using affix decomposition and then falls back\n    to suffix decomposition if affix decomposition fails.\n    \"\"\"\n\n    __slots__ = [\"_greedy\", \"_dictionary_lookup\", \"_greedy_dictionary_lookup\"]\n\n    def __init__(\n        self,\n        greedy: bool,\n        dictionary_lookup: DictionaryLookupStrategy = DictionaryLookupStrategy(),\n        greedy_dictionary_lookup: GreedyDictionaryLookupStrategy = GreedyDictionaryLookupStrategy(),\n    ):\n        \"\"\"\n        Initialize the Affix Decomposition Strategy.\n\n        Args:\n            greedy (bool): Flag indicating whether to use greedy decomposition.\n            dictionary_lookup (DictionaryLookupStrategy): The dictionary lookup strategy to use.\n                Defaults to `DictionaryLookupStrategy()`.\n            greedy_dictionary_lookup (GreedyDictionaryLookupStrategy): The greedy dictionary lookup strategy to use.\n                Defaults to `GreedyDictionaryLookupStrategy()`.\n        \"\"\"\n        self._greedy = greedy\n        self._dictionary_lookup = dictionary_lookup\n        self._greedy_dictionary_lookup = greedy_dictionary_lookup\n\n    def get_lemma(self, token: str, lang: str) -&gt; Optional[str]:\n        \"\"\"\n        Get the lemma of a token using affix decomposition strategy.\n\n        Args:\n            token (str): The input token.\n            lang (str): The language code.\n\n        Returns:\n            Optional[str]: The lemma of the token if found, or None otherwise.\n        \"\"\"\n        limit = 6 if lang in SHORTER_GREEDY else 8\n        if (not self._greedy and lang not in AFFIX_LANGS) or len(token) &lt;= limit:\n            return None\n\n        # define parameters\n        max_affix_len = LONGAFFIXLEN if lang in LONGER_AFFIXES else AFFIXLEN\n        # greedier subword decomposition: suffix search with character in between\n        # then suffixes\n        return self._affix_decomposition(\n            token, lang, max_affix_len, MINCOMPLEN\n        ) or self._suffix_decomposition(token, lang, MINCOMPLEN)\n\n    def _affix_decomposition(\n        self,\n        token: str,\n        lang: str,\n        max_affix_len: int = 0,\n        min_complem_len: int = 0,\n    ) -&gt; Optional[str]:\n        \"\"\"\n        Perform affix decomposition on a token.\n\n        Args:\n            token (str): The input token.\n            lang (str): The language code.\n            max_affix_len (int): The maximum length of the affix.\n            min_complem_len (int): The minimum length of the complementary part.\n\n        Returns:\n            Optional[str]: The lemma of the token if found, or None otherwise.\n        \"\"\"\n        # this only makes sense for languages written from left to right\n        # AFFIXLEN or MINCOMPLEN can spare time for some languages\n        for affixlen in range(max_affix_len, 1, -1):\n            for count in range(1, len(token) - min_complem_len + 1):\n                part1 = token[:-count]\n                # part1_aff = candidate[:-(count + affixlen)]p\n                lempart1 = self._dictionary_lookup.get_lemma(part1, lang)\n                if lempart1 is None:\n                    continue\n                # maybe an affix? discard it\n                if count &lt;= affixlen:\n                    return lempart1\n                # account for case before looking for second part\n                part2 = token[-count:]\n                if token[0].isupper():\n                    part2 = part2.capitalize()\n                lempart2 = self._dictionary_lookup.get_lemma(part2, lang)\n                if lempart2 is None:\n                    continue\n                # candidate must be shorter\n                # try other case\n                candidate = self._greedy_dictionary_lookup.get_lemma(part2, lang)\n                # shorten the second known part of the token\n                if candidate is not None and len(candidate) &lt; len(part2):\n                    return part1 + candidate.lower()\n                # backup: equal length or further candidates accepted\n                # try without capitalizing\n                # even greedier\n                # with capital letter?\n                if len(lempart2) &lt; len(part2) + affixlen:\n                    return part1 + lempart2.lower()\n                    # print(part1, part2, affixlen, count, newcandidate, planb)\n                # elif newcandidate and len(newcandidate) &lt; len(part2) + affixlen:\n                # plan_b = part1 + newcandidate.lower()\n                # print(part1, part2, affixlen, count, newcandidate, planb)\n                # else:\n                #    print(part1, part2, affixlen, count, newcandidate)\n        return None\n\n    def _suffix_decomposition(\n        self,\n        token: str,\n        lang: str,\n        min_complem_len: int = 0,\n    ) -&gt; Optional[str]:\n        \"\"\"\n        Decomposes the token using suffix decomposition strategy.\n\n        Args:\n            token (str): The token to be decomposed.\n            lang (str): The language of the token.\n            min_complem_len (int, optional): The minimum length of the complementary part\n                to consider during decomposition. Defaults to 0.\n\n        Returns:\n            Optional[str]: The decomposed token if decomposition is successful, None otherwise.\n        \"\"\"\n        for count in range(len(token) - min_complem_len, min_complem_len - 1, -1):\n            suffix = self._dictionary_lookup.get_lemma(\n                token[-count:].capitalize(), lang\n            )\n            if suffix is not None and len(suffix) &lt;= len(token[-count:]):\n                return token[:-count] + suffix.lower()\n\n        return None\n</code></pre>"},{"location":"reference/strategies/affix_decomposition/#simplemma.strategies.affix_decomposition.AffixDecompositionStrategy-functions","title":"Functions","text":""},{"location":"reference/strategies/affix_decomposition/#simplemma.strategies.affix_decomposition.AffixDecompositionStrategy.__init__","title":"<code>__init__(greedy, dictionary_lookup=DictionaryLookupStrategy(), greedy_dictionary_lookup=GreedyDictionaryLookupStrategy())</code>","text":"<p>Initialize the Affix Decomposition Strategy.</p> <p>Parameters:</p> Name Type Description Default <code>greedy</code> <code>bool</code> <p>Flag indicating whether to use greedy decomposition.</p> required <code>dictionary_lookup</code> <code>DictionaryLookupStrategy</code> <p>The dictionary lookup strategy to use. Defaults to <code>DictionaryLookupStrategy()</code>.</p> <code>DictionaryLookupStrategy()</code> <code>greedy_dictionary_lookup</code> <code>GreedyDictionaryLookupStrategy</code> <p>The greedy dictionary lookup strategy to use. Defaults to <code>GreedyDictionaryLookupStrategy()</code>.</p> <code>GreedyDictionaryLookupStrategy()</code> Source code in <code>simplemma/strategies/affix_decomposition.py</code> <pre><code>def __init__(\n    self,\n    greedy: bool,\n    dictionary_lookup: DictionaryLookupStrategy = DictionaryLookupStrategy(),\n    greedy_dictionary_lookup: GreedyDictionaryLookupStrategy = GreedyDictionaryLookupStrategy(),\n):\n    \"\"\"\n    Initialize the Affix Decomposition Strategy.\n\n    Args:\n        greedy (bool): Flag indicating whether to use greedy decomposition.\n        dictionary_lookup (DictionaryLookupStrategy): The dictionary lookup strategy to use.\n            Defaults to `DictionaryLookupStrategy()`.\n        greedy_dictionary_lookup (GreedyDictionaryLookupStrategy): The greedy dictionary lookup strategy to use.\n            Defaults to `GreedyDictionaryLookupStrategy()`.\n    \"\"\"\n    self._greedy = greedy\n    self._dictionary_lookup = dictionary_lookup\n    self._greedy_dictionary_lookup = greedy_dictionary_lookup\n</code></pre>"},{"location":"reference/strategies/affix_decomposition/#simplemma.strategies.affix_decomposition.AffixDecompositionStrategy.get_lemma","title":"<code>get_lemma(token, lang)</code>","text":"<p>Get the lemma of a token using affix decomposition strategy.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>str</code> <p>The input token.</p> required <code>lang</code> <code>str</code> <p>The language code.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The lemma of the token if found, or None otherwise.</p> Source code in <code>simplemma/strategies/affix_decomposition.py</code> <pre><code>def get_lemma(self, token: str, lang: str) -&gt; Optional[str]:\n    \"\"\"\n    Get the lemma of a token using affix decomposition strategy.\n\n    Args:\n        token (str): The input token.\n        lang (str): The language code.\n\n    Returns:\n        Optional[str]: The lemma of the token if found, or None otherwise.\n    \"\"\"\n    limit = 6 if lang in SHORTER_GREEDY else 8\n    if (not self._greedy and lang not in AFFIX_LANGS) or len(token) &lt;= limit:\n        return None\n\n    # define parameters\n    max_affix_len = LONGAFFIXLEN if lang in LONGER_AFFIXES else AFFIXLEN\n    # greedier subword decomposition: suffix search with character in between\n    # then suffixes\n    return self._affix_decomposition(\n        token, lang, max_affix_len, MINCOMPLEN\n    ) or self._suffix_decomposition(token, lang, MINCOMPLEN)\n</code></pre>"},{"location":"reference/strategies/default/","title":"Default Strategy","text":"<p>This module defines the <code>DefaultStrategy</code> class, which is a concrete implementation of the <code>LemmatizationStrategy</code> protocol. It provides lemmatization using a combination of different strategies such as dictionary lookup, hyphen removal, rule-based lemmatization, prefix decomposition, and affix decomposition.</p>"},{"location":"reference/strategies/default/#simplemma.strategies.default-classes","title":"Classes","text":""},{"location":"reference/strategies/default/#simplemma.strategies.default.DefaultStrategy","title":"<code>DefaultStrategy</code>","text":"<p>             Bases: <code>LemmatizationStrategy</code></p> <p>This class represents a lemmatization strategy that combines different techniques to perform lemmatization. It implements the <code>LemmatizationStrategy</code> protocol.</p> Source code in <code>simplemma/strategies/default.py</code> <pre><code>class DefaultStrategy(LemmatizationStrategy):\n    \"\"\"\n    This class represents a lemmatization strategy that combines different techniques to perform lemmatization.\n    It implements the `LemmatizationStrategy` protocol.\n    \"\"\"\n\n    __slots__ = [\n        \"_dictionary_lookup\",\n        \"_hyphen_search\",\n        \"_rules_search\",\n        \"_prefix_search\",\n        \"_greedy_dictionary_lookup\",\n        \"_affix_search\",\n    ]\n\n    def __init__(\n        self,\n        greedy: bool = False,\n        dictionary_factory: DictionaryFactory = DefaultDictionaryFactory(),\n    ):\n        \"\"\"\n        Initialize the Default Strategy.\n\n        Args:\n            greedy (bool): Whether to use a greedy approach for dictionary lookup. Defaults to `False`.\n            dictionary_factory (DictionaryFactory): A factory for creating dictionaries.\n                Defaults to [`DefaultDictionaryFactory()`][simplemma.strategies.dictionaries.dictionary_factory.DefaultDictionaryFactory]..\n\n        \"\"\"\n        self._greedy = greedy\n        self._dictionary_lookup = DictionaryLookupStrategy(dictionary_factory)\n        self._hyphen_search = HyphenRemovalStrategy(self._dictionary_lookup)\n        self._rules_search = RulesStrategy()\n        self._prefix_search = PrefixDecompositionStrategy(\n            dictionary_lookup=self._dictionary_lookup\n        )\n        greedy_dictionary_lookup = GreedyDictionaryLookupStrategy(dictionary_factory)\n        self._affix_search = AffixDecompositionStrategy(\n            greedy, self._dictionary_lookup, greedy_dictionary_lookup\n        )\n\n        self._greedy_dictionary_lookup = greedy_dictionary_lookup if greedy else None\n\n    def get_lemma(self, token: str, lang: str) -&gt; Optional[str]:\n        \"\"\"\n        Get the lemma for a given token and language using the combination of different lemmatization techniques.\n\n        Args:\n            token (str): The token to lemmatize.\n            lang (str): The language of the token.\n\n        Returns:\n            Optional[str]: The lemma of the token, or None if no lemma is found.\n\n        \"\"\"\n        # filters\n        if token.isnumeric():\n            return token\n\n        candidate = (\n            # supervised searches\n            self._dictionary_lookup.get_lemma(token, lang)\n            or self._hyphen_search.get_lemma(token, lang)\n            or self._rules_search.get_lemma(token, lang)\n            or self._prefix_search.get_lemma(token, lang)\n            # weakly supervised / greedier searches\n            or self._affix_search.get_lemma(token, lang)\n        )\n\n        # additional round\n        if candidate is not None and self._greedy_dictionary_lookup is not None:\n            candidate = self._greedy_dictionary_lookup.get_lemma(candidate, lang)\n\n        return candidate\n</code></pre>"},{"location":"reference/strategies/default/#simplemma.strategies.default.DefaultStrategy-functions","title":"Functions","text":""},{"location":"reference/strategies/default/#simplemma.strategies.default.DefaultStrategy.__init__","title":"<code>__init__(greedy=False, dictionary_factory=DefaultDictionaryFactory())</code>","text":"<p>Initialize the Default Strategy.</p> <p>Parameters:</p> Name Type Description Default <code>greedy</code> <code>bool</code> <p>Whether to use a greedy approach for dictionary lookup. Defaults to <code>False</code>.</p> <code>False</code> <code>dictionary_factory</code> <code>DictionaryFactory</code> <p>A factory for creating dictionaries. Defaults to <code>DefaultDictionaryFactory()</code>..</p> <code>DefaultDictionaryFactory()</code> Source code in <code>simplemma/strategies/default.py</code> <pre><code>def __init__(\n    self,\n    greedy: bool = False,\n    dictionary_factory: DictionaryFactory = DefaultDictionaryFactory(),\n):\n    \"\"\"\n    Initialize the Default Strategy.\n\n    Args:\n        greedy (bool): Whether to use a greedy approach for dictionary lookup. Defaults to `False`.\n        dictionary_factory (DictionaryFactory): A factory for creating dictionaries.\n            Defaults to [`DefaultDictionaryFactory()`][simplemma.strategies.dictionaries.dictionary_factory.DefaultDictionaryFactory]..\n\n    \"\"\"\n    self._greedy = greedy\n    self._dictionary_lookup = DictionaryLookupStrategy(dictionary_factory)\n    self._hyphen_search = HyphenRemovalStrategy(self._dictionary_lookup)\n    self._rules_search = RulesStrategy()\n    self._prefix_search = PrefixDecompositionStrategy(\n        dictionary_lookup=self._dictionary_lookup\n    )\n    greedy_dictionary_lookup = GreedyDictionaryLookupStrategy(dictionary_factory)\n    self._affix_search = AffixDecompositionStrategy(\n        greedy, self._dictionary_lookup, greedy_dictionary_lookup\n    )\n\n    self._greedy_dictionary_lookup = greedy_dictionary_lookup if greedy else None\n</code></pre>"},{"location":"reference/strategies/default/#simplemma.strategies.default.DefaultStrategy.get_lemma","title":"<code>get_lemma(token, lang)</code>","text":"<p>Get the lemma for a given token and language using the combination of different lemmatization techniques.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>str</code> <p>The token to lemmatize.</p> required <code>lang</code> <code>str</code> <p>The language of the token.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The lemma of the token, or None if no lemma is found.</p> Source code in <code>simplemma/strategies/default.py</code> <pre><code>def get_lemma(self, token: str, lang: str) -&gt; Optional[str]:\n    \"\"\"\n    Get the lemma for a given token and language using the combination of different lemmatization techniques.\n\n    Args:\n        token (str): The token to lemmatize.\n        lang (str): The language of the token.\n\n    Returns:\n        Optional[str]: The lemma of the token, or None if no lemma is found.\n\n    \"\"\"\n    # filters\n    if token.isnumeric():\n        return token\n\n    candidate = (\n        # supervised searches\n        self._dictionary_lookup.get_lemma(token, lang)\n        or self._hyphen_search.get_lemma(token, lang)\n        or self._rules_search.get_lemma(token, lang)\n        or self._prefix_search.get_lemma(token, lang)\n        # weakly supervised / greedier searches\n        or self._affix_search.get_lemma(token, lang)\n    )\n\n    # additional round\n    if candidate is not None and self._greedy_dictionary_lookup is not None:\n        candidate = self._greedy_dictionary_lookup.get_lemma(candidate, lang)\n\n    return candidate\n</code></pre>"},{"location":"reference/strategies/dictionary_lookup/","title":"Dictionary Lookup Strategy","text":"<p>This module defines the <code>DictionaryLookupStrategy</code> class, which is a concrete implementation of the <code>LemmatizationStrategy</code> protocol. It provides lemmatization using dictionary lookup.</p>"},{"location":"reference/strategies/dictionary_lookup/#simplemma.strategies.dictionary_lookup-classes","title":"Classes","text":""},{"location":"reference/strategies/dictionary_lookup/#simplemma.strategies.dictionary_lookup.DictionaryLookupStrategy","title":"<code>DictionaryLookupStrategy</code>","text":"<p>             Bases: <code>LemmatizationStrategy</code></p> <p>Dictionary Lookup Strategy</p> Source code in <code>simplemma/strategies/dictionary_lookup.py</code> <pre><code>class DictionaryLookupStrategy(LemmatizationStrategy):\n    \"\"\"Dictionary Lookup Strategy\"\"\"\n\n    __slots__ = [\"_dictionary_factory\"]\n\n    def __init__(\n        self, dictionary_factory: DictionaryFactory = DefaultDictionaryFactory()\n    ):\n        \"\"\"\n        Initialize the Dictionary Lookup Strategy.\n\n        Args:\n            dictionary_factory (DictionaryFactory): The dictionary factory used to obtain language dictionaries.\n                Defaults to [`DefaultDictionaryFactory()`][simplemma.strategies.dictionaries.dictionary_factory.DefaultDictionaryFactory].\n        \"\"\"\n        self._dictionary_factory = dictionary_factory\n\n    def get_lemma(self, token: str, lang: str) -&gt; Optional[str]:\n        \"\"\"\n        Get Lemma using Dictionary Lookup\n\n        This method performs lemmatization by looking up the token in the language-specific dictionary.\n        It returns the lemma if found, or `None` if not found.\n\n        Args:\n            token (str): The input token to lemmatize.\n            lang (str): The language code for the token's language.\n\n        Returns:\n            Optional[str]: The lemma for the token, or `None` if not found in the dictionary.\n\n        \"\"\"\n        # Search the language data, reverse case to extend coverage.\n        dictionary = self._dictionary_factory.get_dictionary(lang)\n        if result := dictionary.get(token):\n            return result\n        # Try upper or lowercase.\n        token = token.lower() if token[0].isupper() else token.capitalize()\n        return dictionary.get(token)\n</code></pre>"},{"location":"reference/strategies/dictionary_lookup/#simplemma.strategies.dictionary_lookup.DictionaryLookupStrategy-functions","title":"Functions","text":""},{"location":"reference/strategies/dictionary_lookup/#simplemma.strategies.dictionary_lookup.DictionaryLookupStrategy.__init__","title":"<code>__init__(dictionary_factory=DefaultDictionaryFactory())</code>","text":"<p>Initialize the Dictionary Lookup Strategy.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_factory</code> <code>DictionaryFactory</code> <p>The dictionary factory used to obtain language dictionaries. Defaults to <code>DefaultDictionaryFactory()</code>.</p> <code>DefaultDictionaryFactory()</code> Source code in <code>simplemma/strategies/dictionary_lookup.py</code> <pre><code>def __init__(\n    self, dictionary_factory: DictionaryFactory = DefaultDictionaryFactory()\n):\n    \"\"\"\n    Initialize the Dictionary Lookup Strategy.\n\n    Args:\n        dictionary_factory (DictionaryFactory): The dictionary factory used to obtain language dictionaries.\n            Defaults to [`DefaultDictionaryFactory()`][simplemma.strategies.dictionaries.dictionary_factory.DefaultDictionaryFactory].\n    \"\"\"\n    self._dictionary_factory = dictionary_factory\n</code></pre>"},{"location":"reference/strategies/dictionary_lookup/#simplemma.strategies.dictionary_lookup.DictionaryLookupStrategy.get_lemma","title":"<code>get_lemma(token, lang)</code>","text":"<p>Get Lemma using Dictionary Lookup</p> <p>This method performs lemmatization by looking up the token in the language-specific dictionary. It returns the lemma if found, or <code>None</code> if not found.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>str</code> <p>The input token to lemmatize.</p> required <code>lang</code> <code>str</code> <p>The language code for the token's language.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The lemma for the token, or <code>None</code> if not found in the dictionary.</p> Source code in <code>simplemma/strategies/dictionary_lookup.py</code> <pre><code>def get_lemma(self, token: str, lang: str) -&gt; Optional[str]:\n    \"\"\"\n    Get Lemma using Dictionary Lookup\n\n    This method performs lemmatization by looking up the token in the language-specific dictionary.\n    It returns the lemma if found, or `None` if not found.\n\n    Args:\n        token (str): The input token to lemmatize.\n        lang (str): The language code for the token's language.\n\n    Returns:\n        Optional[str]: The lemma for the token, or `None` if not found in the dictionary.\n\n    \"\"\"\n    # Search the language data, reverse case to extend coverage.\n    dictionary = self._dictionary_factory.get_dictionary(lang)\n    if result := dictionary.get(token):\n        return result\n    # Try upper or lowercase.\n    token = token.lower() if token[0].isupper() else token.capitalize()\n    return dictionary.get(token)\n</code></pre>"},{"location":"reference/strategies/greedy_dictionary_lookup/","title":"Greedy Dictionary Lookup Strategy","text":"<p>This module defines the <code>GreedyDictionaryLookupStrategy</code> class, which is a concrete implementation of the <code>LemmatizationStrategy</code> protocol. It provides lemmatization using a greedy dictionary lookup strategy.</p>"},{"location":"reference/strategies/greedy_dictionary_lookup/#simplemma.strategies.greedy_dictionary_lookup-classes","title":"Classes","text":""},{"location":"reference/strategies/greedy_dictionary_lookup/#simplemma.strategies.greedy_dictionary_lookup.GreedyDictionaryLookupStrategy","title":"<code>GreedyDictionaryLookupStrategy</code>","text":"<p>             Bases: <code>LemmatizationStrategy</code></p> <p>This class represents a lemmatization strategy that performs lemmatization using a greedy dictionary lookup strategy.</p> Source code in <code>simplemma/strategies/greedy_dictionary_lookup.py</code> <pre><code>class GreedyDictionaryLookupStrategy(LemmatizationStrategy):\n    \"\"\"\n    This class represents a lemmatization strategy that performs lemmatization using a greedy dictionary lookup strategy.\n    \"\"\"\n\n    __slots__ = [\"_dictionary_factory\", \"_distance\", \"_steps\"]\n\n    def __init__(\n        self,\n        dictionary_factory: DictionaryFactory = DefaultDictionaryFactory(),\n        steps: int = 1,\n        distance: int = 5,\n    ):\n        \"\"\"\n        Initialize the Greedy Dictionary Lookup Strategy.\n\n        Args:\n            dictionary_factory (DictionaryFactory): The dictionary factory used to obtain language dictionaries.\n                Defaults to [`DefaultDictionaryFactory()`][simplemma.strategies.dictionaries.dictionary_factory.DefaultDictionaryFactory]..\n            steps (int): The maximum number of lemmatization steps to perform. Defaults to `1`.\n            distance (int): The maximum allowed Levenshtein distance between candidate lemmas. Defaults to `5`.\n\n        \"\"\"\n        self._dictionary_factory = dictionary_factory\n        self._steps = steps\n        self._distance = distance\n\n    def get_lemma(self, token: str, lang: str) -&gt; str:\n        \"\"\"\n        Get Lemma using Greedy Dictionary Lookup Strategy\n\n        This method performs lemmatization by looking up the token in the language-specific dictionary using a greedy strategy.\n        It iteratively applies the dictionary lookup and checks the candidate lemmas based on length and Levenshtein distance.\n        It returns the resulting lemma after the specified number of steps or when the conditions are not met.\n\n        Args:\n            token (str): The input token to lemmatize.\n            lang (str): The language code for the token's language.\n\n        Returns:\n            str: The lemma for the token.\n\n        \"\"\"\n        limit = 6 if lang in SHORTER_GREEDY else 8\n        if len(token) &lt;= limit:\n            return token\n\n        dictionary = self._dictionary_factory.get_dictionary(lang)\n\n        for _ in range(self._steps):\n            candidate = dictionary.get(token)\n\n            if (\n                not candidate\n                or len(candidate) &gt; len(token)\n                or levenshtein_dist(candidate, token) &gt; self._distance\n            ):\n                break\n\n            token = candidate\n\n        return token\n</code></pre>"},{"location":"reference/strategies/greedy_dictionary_lookup/#simplemma.strategies.greedy_dictionary_lookup.GreedyDictionaryLookupStrategy-functions","title":"Functions","text":""},{"location":"reference/strategies/greedy_dictionary_lookup/#simplemma.strategies.greedy_dictionary_lookup.GreedyDictionaryLookupStrategy.__init__","title":"<code>__init__(dictionary_factory=DefaultDictionaryFactory(), steps=1, distance=5)</code>","text":"<p>Initialize the Greedy Dictionary Lookup Strategy.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_factory</code> <code>DictionaryFactory</code> <p>The dictionary factory used to obtain language dictionaries. Defaults to <code>DefaultDictionaryFactory()</code>..</p> <code>DefaultDictionaryFactory()</code> <code>steps</code> <code>int</code> <p>The maximum number of lemmatization steps to perform. Defaults to <code>1</code>.</p> <code>1</code> <code>distance</code> <code>int</code> <p>The maximum allowed Levenshtein distance between candidate lemmas. Defaults to <code>5</code>.</p> <code>5</code> Source code in <code>simplemma/strategies/greedy_dictionary_lookup.py</code> <pre><code>def __init__(\n    self,\n    dictionary_factory: DictionaryFactory = DefaultDictionaryFactory(),\n    steps: int = 1,\n    distance: int = 5,\n):\n    \"\"\"\n    Initialize the Greedy Dictionary Lookup Strategy.\n\n    Args:\n        dictionary_factory (DictionaryFactory): The dictionary factory used to obtain language dictionaries.\n            Defaults to [`DefaultDictionaryFactory()`][simplemma.strategies.dictionaries.dictionary_factory.DefaultDictionaryFactory]..\n        steps (int): The maximum number of lemmatization steps to perform. Defaults to `1`.\n        distance (int): The maximum allowed Levenshtein distance between candidate lemmas. Defaults to `5`.\n\n    \"\"\"\n    self._dictionary_factory = dictionary_factory\n    self._steps = steps\n    self._distance = distance\n</code></pre>"},{"location":"reference/strategies/greedy_dictionary_lookup/#simplemma.strategies.greedy_dictionary_lookup.GreedyDictionaryLookupStrategy.get_lemma","title":"<code>get_lemma(token, lang)</code>","text":"<p>Get Lemma using Greedy Dictionary Lookup Strategy</p> <p>This method performs lemmatization by looking up the token in the language-specific dictionary using a greedy strategy. It iteratively applies the dictionary lookup and checks the candidate lemmas based on length and Levenshtein distance. It returns the resulting lemma after the specified number of steps or when the conditions are not met.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>str</code> <p>The input token to lemmatize.</p> required <code>lang</code> <code>str</code> <p>The language code for the token's language.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The lemma for the token.</p> Source code in <code>simplemma/strategies/greedy_dictionary_lookup.py</code> <pre><code>def get_lemma(self, token: str, lang: str) -&gt; str:\n    \"\"\"\n    Get Lemma using Greedy Dictionary Lookup Strategy\n\n    This method performs lemmatization by looking up the token in the language-specific dictionary using a greedy strategy.\n    It iteratively applies the dictionary lookup and checks the candidate lemmas based on length and Levenshtein distance.\n    It returns the resulting lemma after the specified number of steps or when the conditions are not met.\n\n    Args:\n        token (str): The input token to lemmatize.\n        lang (str): The language code for the token's language.\n\n    Returns:\n        str: The lemma for the token.\n\n    \"\"\"\n    limit = 6 if lang in SHORTER_GREEDY else 8\n    if len(token) &lt;= limit:\n        return token\n\n    dictionary = self._dictionary_factory.get_dictionary(lang)\n\n    for _ in range(self._steps):\n        candidate = dictionary.get(token)\n\n        if (\n            not candidate\n            or len(candidate) &gt; len(token)\n            or levenshtein_dist(candidate, token) &gt; self._distance\n        ):\n            break\n\n        token = candidate\n\n    return token\n</code></pre>"},{"location":"reference/strategies/greedy_dictionary_lookup/#simplemma.strategies.greedy_dictionary_lookup-functions","title":"Functions","text":""},{"location":"reference/strategies/hyphen_removal/","title":"Hyphen removal Strategy","text":"<p>This module defines the <code>HyphenRemovalStrategy</code> class, which is a concrete implementation of the <code>LemmatizationStrategy</code> protocol. It provides lemmatization by removing hyphens from tokens and attempting to find dictionary forms.</p>"},{"location":"reference/strategies/hyphen_removal/#simplemma.strategies.hyphen_removal-classes","title":"Classes","text":""},{"location":"reference/strategies/hyphen_removal/#simplemma.strategies.hyphen_removal.HyphenRemovalStrategy","title":"<code>HyphenRemovalStrategy</code>","text":"<p>             Bases: <code>LemmatizationStrategy</code></p> <p>This class represents a lemmatization strategy that performs lemmatization by removing hyphens from tokens and attempting to find dictionary forms. It implements the <code>LemmatizationStrategy</code> protocol.</p> Source code in <code>simplemma/strategies/hyphen_removal.py</code> <pre><code>class HyphenRemovalStrategy(LemmatizationStrategy):\n    \"\"\"\n    This class represents a lemmatization strategy that performs lemmatization by removing hyphens from tokens\n    and attempting to find dictionary forms.\n    It implements the `LemmatizationStrategy` protocol.\n    \"\"\"\n\n    __slots__ = [\"_dictionary_lookup\"]\n\n    def __init__(\n        self, dictionary_lookup: DictionaryLookupStrategy = DictionaryLookupStrategy()\n    ):\n        \"\"\"\n        Initialize the Hyphen Removal Strategy.\n\n        Args:\n            dictionary_lookup (DictionaryLookupStrategy): The dictionary lookup strategy used to find dictionary forms.\n                Defaults to `DictionaryLookupStrategy()`.\n\n        \"\"\"\n        self._dictionary_lookup = dictionary_lookup\n\n    def get_lemma(self, token: str, lang: str) -&gt; Optional[str]:\n        \"\"\"\n        Get Lemma using Hyphen Removal Strategy\n\n        This method performs lemmatization by removing hyphens from the token and attempting to find a dictionary form.\n        It splits the token based on hyphen characters, removes hyphens, and forms a candidate lemma for lookup.\n        If a dictionary form is found, it is returned as the lemma.\n        If not found, it attempts to decompose the token by looking up the last part (after the last hyphen) in the dictionary.\n        If a lemma is found for the last part, it replaces the last part in the token and returns the modified token as the lemma.\n        If no dictionary form is found, None is returned.\n\n        Args:\n            token (str): The input token to lemmatize.\n            lang (str): The language code for the token's language.\n\n        Returns:\n            Optional[str]: The lemma for the token, or None if no lemma is found.\n\n        \"\"\"\n        token_parts = HYPHEN_REGEX.split(token)\n        if len(token_parts) &lt;= 1 or not token_parts[-1]:\n            return None\n\n        # try to find a word form without hyphen\n        candidate = \"\".join([t for t in token_parts if t not in HYPHENS]).lower()\n        if token[0].isupper():\n            candidate = candidate.capitalize()\n\n        lemma = self._dictionary_lookup.get_lemma(candidate, lang)\n        if lemma is not None:\n            return lemma\n\n        # decompose\n        last_part_lemma = self._dictionary_lookup.get_lemma(token_parts[-1], lang)\n        if last_part_lemma is not None:\n            return \"\".join(token_parts[:-1] + [last_part_lemma])\n\n        return None\n</code></pre>"},{"location":"reference/strategies/hyphen_removal/#simplemma.strategies.hyphen_removal.HyphenRemovalStrategy-functions","title":"Functions","text":""},{"location":"reference/strategies/hyphen_removal/#simplemma.strategies.hyphen_removal.HyphenRemovalStrategy.__init__","title":"<code>__init__(dictionary_lookup=DictionaryLookupStrategy())</code>","text":"<p>Initialize the Hyphen Removal Strategy.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_lookup</code> <code>DictionaryLookupStrategy</code> <p>The dictionary lookup strategy used to find dictionary forms. Defaults to <code>DictionaryLookupStrategy()</code>.</p> <code>DictionaryLookupStrategy()</code> Source code in <code>simplemma/strategies/hyphen_removal.py</code> <pre><code>def __init__(\n    self, dictionary_lookup: DictionaryLookupStrategy = DictionaryLookupStrategy()\n):\n    \"\"\"\n    Initialize the Hyphen Removal Strategy.\n\n    Args:\n        dictionary_lookup (DictionaryLookupStrategy): The dictionary lookup strategy used to find dictionary forms.\n            Defaults to `DictionaryLookupStrategy()`.\n\n    \"\"\"\n    self._dictionary_lookup = dictionary_lookup\n</code></pre>"},{"location":"reference/strategies/hyphen_removal/#simplemma.strategies.hyphen_removal.HyphenRemovalStrategy.get_lemma","title":"<code>get_lemma(token, lang)</code>","text":"<p>Get Lemma using Hyphen Removal Strategy</p> <p>This method performs lemmatization by removing hyphens from the token and attempting to find a dictionary form. It splits the token based on hyphen characters, removes hyphens, and forms a candidate lemma for lookup. If a dictionary form is found, it is returned as the lemma. If not found, it attempts to decompose the token by looking up the last part (after the last hyphen) in the dictionary. If a lemma is found for the last part, it replaces the last part in the token and returns the modified token as the lemma. If no dictionary form is found, None is returned.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>str</code> <p>The input token to lemmatize.</p> required <code>lang</code> <code>str</code> <p>The language code for the token's language.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The lemma for the token, or None if no lemma is found.</p> Source code in <code>simplemma/strategies/hyphen_removal.py</code> <pre><code>def get_lemma(self, token: str, lang: str) -&gt; Optional[str]:\n    \"\"\"\n    Get Lemma using Hyphen Removal Strategy\n\n    This method performs lemmatization by removing hyphens from the token and attempting to find a dictionary form.\n    It splits the token based on hyphen characters, removes hyphens, and forms a candidate lemma for lookup.\n    If a dictionary form is found, it is returned as the lemma.\n    If not found, it attempts to decompose the token by looking up the last part (after the last hyphen) in the dictionary.\n    If a lemma is found for the last part, it replaces the last part in the token and returns the modified token as the lemma.\n    If no dictionary form is found, None is returned.\n\n    Args:\n        token (str): The input token to lemmatize.\n        lang (str): The language code for the token's language.\n\n    Returns:\n        Optional[str]: The lemma for the token, or None if no lemma is found.\n\n    \"\"\"\n    token_parts = HYPHEN_REGEX.split(token)\n    if len(token_parts) &lt;= 1 or not token_parts[-1]:\n        return None\n\n    # try to find a word form without hyphen\n    candidate = \"\".join([t for t in token_parts if t not in HYPHENS]).lower()\n    if token[0].isupper():\n        candidate = candidate.capitalize()\n\n    lemma = self._dictionary_lookup.get_lemma(candidate, lang)\n    if lemma is not None:\n        return lemma\n\n    # decompose\n    last_part_lemma = self._dictionary_lookup.get_lemma(token_parts[-1], lang)\n    if last_part_lemma is not None:\n        return \"\".join(token_parts[:-1] + [last_part_lemma])\n\n    return None\n</code></pre>"},{"location":"reference/strategies/lemmatization_strategy/","title":"Lemmatization Strategy","text":"<p>This file defines the <code>LemmatizationStrategy</code> protocl class, which all lemmatization strategies should extend to be usable by the Simplemma library.</p>"},{"location":"reference/strategies/lemmatization_strategy/#simplemma.strategies.lemmatization_strategy-classes","title":"Classes","text":""},{"location":"reference/strategies/lemmatization_strategy/#simplemma.strategies.lemmatization_strategy.LemmatizationStrategy","title":"<code>LemmatizationStrategy</code>","text":"<p>             Bases: <code>Protocol</code></p> <p>This protocol defines the interface for lemmatization strategies. Subclasses implementing this protocol must provide an implementation for the <code>get_lemma</code> method.</p> Note <p>This protocol should be implemented by concrete lemmatization strategy classes. Concrete implementations of this protocol should provide a concrete implementation for the <code>get_lemma</code> method.</p> Source code in <code>simplemma/strategies/lemmatization_strategy.py</code> <pre><code>class LemmatizationStrategy(Protocol):\n    \"\"\"\n    This protocol defines the interface for lemmatization strategies. Subclasses implementing this protocol\n    must provide an implementation for the `get_lemma` method.\n\n    Note:\n        This protocol should be implemented by concrete lemmatization strategy classes.\n        Concrete implementations of this protocol should provide a concrete implementation for the `get_lemma` method.\n    \"\"\"\n\n    @abstractmethod\n    def get_lemma(self, token: str, lang: str) -&gt; Optional[str]:\n        \"\"\"\n        This method receives a token and a language code and should return the lemma for the token in the specified language.\n        If the lemma is not found, it should return `None`.\n\n        Args:\n            token (str): The input token to lemmatize.\n            lang (str): The language code for the token's language.\n\n        Returns:\n            Optional[str]: The lemma for the token, or `None` if not found.\n\n        Raises:\n            NotImplementedError: If the method is not implemented by the subclass.\n\n        \"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference/strategies/lemmatization_strategy/#simplemma.strategies.lemmatization_strategy.LemmatizationStrategy-functions","title":"Functions","text":""},{"location":"reference/strategies/lemmatization_strategy/#simplemma.strategies.lemmatization_strategy.LemmatizationStrategy.get_lemma","title":"<code>get_lemma(token, lang)</code>  <code>abstractmethod</code>","text":"<p>This method receives a token and a language code and should return the lemma for the token in the specified language. If the lemma is not found, it should return <code>None</code>.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>str</code> <p>The input token to lemmatize.</p> required <code>lang</code> <code>str</code> <p>The language code for the token's language.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The lemma for the token, or <code>None</code> if not found.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented by the subclass.</p> Source code in <code>simplemma/strategies/lemmatization_strategy.py</code> <pre><code>@abstractmethod\ndef get_lemma(self, token: str, lang: str) -&gt; Optional[str]:\n    \"\"\"\n    This method receives a token and a language code and should return the lemma for the token in the specified language.\n    If the lemma is not found, it should return `None`.\n\n    Args:\n        token (str): The input token to lemmatize.\n        lang (str): The language code for the token's language.\n\n    Returns:\n        Optional[str]: The lemma for the token, or `None` if not found.\n\n    Raises:\n        NotImplementedError: If the method is not implemented by the subclass.\n\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/strategies/prefix_decomposition/","title":"Prefix Decomposition Strategy","text":"<p>This module defines the <code>PrefixDecompositionStrategy</code> class, which is a concrete implementation of the <code>LemmatizationStrategy</code> protocol. It provides lemmatization by performing subword decomposition using pre-defined prefixes.</p>"},{"location":"reference/strategies/prefix_decomposition/#simplemma.strategies.prefix_decomposition-classes","title":"Classes","text":""},{"location":"reference/strategies/prefix_decomposition/#simplemma.strategies.prefix_decomposition.PrefixDecompositionStrategy","title":"<code>PrefixDecompositionStrategy</code>","text":"<p>             Bases: <code>LemmatizationStrategy</code></p> <p>This class represents a lemmatization strategy that performs lemmatization by performing subword decomposition using pre-defined prefixes. It implements the <code>LemmatizationStrategy</code> protocol.</p> Source code in <code>simplemma/strategies/prefix_decomposition.py</code> <pre><code>class PrefixDecompositionStrategy(LemmatizationStrategy):\n    \"\"\"\n    This class represents a lemmatization strategy that performs lemmatization by performing subword decomposition using pre-defined prefixes.\n    It implements the `LemmatizationStrategy` protocol.\n    \"\"\"\n\n    __slots__ = [\"_known_prefixes\", \"_dictionary_lookup\"]\n\n    def __init__(\n        self,\n        known_prefixes: Dict[str, Pattern[str]] = DEFAULT_KNOWN_PREFIXES,\n        dictionary_lookup: DictionaryLookupStrategy = DictionaryLookupStrategy(),\n    ):\n        \"\"\"\n        Initialize the Prefix Decomposition Strategy.\n\n        Args:\n            known_prefixes (Dict[str, Pattern[str]]): A dictionary of known prefixes for various languages.\n                Defaults to `DEFAULT_KNOWN_PREFIXES`.\n            dictionary_lookup (DictionaryLookupStrategy): The dictionary lookup strategy used to find dictionary forms.\n                Defaults to `DictionaryLookupStrategy()`.\n\n        \"\"\"\n        self._known_prefixes = known_prefixes\n        self._dictionary_lookup = dictionary_lookup\n\n    def get_lemma(self, token: str, lang: str) -&gt; Optional[str]:\n        \"\"\"\n        Get Lemma using Prefix Decomposition Strategy\n\n        This method performs lemmatization by performing subword decomposition using pre-defined prefixes.\n        It checks if the language has known prefixes defined.\n        If a known prefix is found at the start of the token, it extracts the prefix and performs dictionary lookup on the remaining subword.\n        If a lemma is found for the subword, it returns the concatenation of the prefix and the lowercase subword.\n        If no known prefix is found or no lemma is found for the subword, None is returned.\n\n        Args:\n            token (str): The input token to lemmatize.\n            lang (str): The language code for the token's language.\n\n        Returns:\n            Optional[str]: The lemma for the token, or None if no lemma is found.\n\n        \"\"\"\n        if lang not in self._known_prefixes:\n            return None\n\n        prefix_match = self._known_prefixes[lang].match(token)\n        if not prefix_match or prefix_match[1] == token:\n            return None\n\n        prefix = prefix_match[1]\n\n        subword = self._dictionary_lookup.get_lemma(token[len(prefix) :], lang)\n\n        return prefix + subword.lower() if subword else None\n</code></pre>"},{"location":"reference/strategies/prefix_decomposition/#simplemma.strategies.prefix_decomposition.PrefixDecompositionStrategy-functions","title":"Functions","text":""},{"location":"reference/strategies/prefix_decomposition/#simplemma.strategies.prefix_decomposition.PrefixDecompositionStrategy.__init__","title":"<code>__init__(known_prefixes=DEFAULT_KNOWN_PREFIXES, dictionary_lookup=DictionaryLookupStrategy())</code>","text":"<p>Initialize the Prefix Decomposition Strategy.</p> <p>Parameters:</p> Name Type Description Default <code>known_prefixes</code> <code>Dict[str, Pattern[str]]</code> <p>A dictionary of known prefixes for various languages. Defaults to <code>DEFAULT_KNOWN_PREFIXES</code>.</p> <code>DEFAULT_KNOWN_PREFIXES</code> <code>dictionary_lookup</code> <code>DictionaryLookupStrategy</code> <p>The dictionary lookup strategy used to find dictionary forms. Defaults to <code>DictionaryLookupStrategy()</code>.</p> <code>DictionaryLookupStrategy()</code> Source code in <code>simplemma/strategies/prefix_decomposition.py</code> <pre><code>def __init__(\n    self,\n    known_prefixes: Dict[str, Pattern[str]] = DEFAULT_KNOWN_PREFIXES,\n    dictionary_lookup: DictionaryLookupStrategy = DictionaryLookupStrategy(),\n):\n    \"\"\"\n    Initialize the Prefix Decomposition Strategy.\n\n    Args:\n        known_prefixes (Dict[str, Pattern[str]]): A dictionary of known prefixes for various languages.\n            Defaults to `DEFAULT_KNOWN_PREFIXES`.\n        dictionary_lookup (DictionaryLookupStrategy): The dictionary lookup strategy used to find dictionary forms.\n            Defaults to `DictionaryLookupStrategy()`.\n\n    \"\"\"\n    self._known_prefixes = known_prefixes\n    self._dictionary_lookup = dictionary_lookup\n</code></pre>"},{"location":"reference/strategies/prefix_decomposition/#simplemma.strategies.prefix_decomposition.PrefixDecompositionStrategy.get_lemma","title":"<code>get_lemma(token, lang)</code>","text":"<p>Get Lemma using Prefix Decomposition Strategy</p> <p>This method performs lemmatization by performing subword decomposition using pre-defined prefixes. It checks if the language has known prefixes defined. If a known prefix is found at the start of the token, it extracts the prefix and performs dictionary lookup on the remaining subword. If a lemma is found for the subword, it returns the concatenation of the prefix and the lowercase subword. If no known prefix is found or no lemma is found for the subword, None is returned.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>str</code> <p>The input token to lemmatize.</p> required <code>lang</code> <code>str</code> <p>The language code for the token's language.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The lemma for the token, or None if no lemma is found.</p> Source code in <code>simplemma/strategies/prefix_decomposition.py</code> <pre><code>def get_lemma(self, token: str, lang: str) -&gt; Optional[str]:\n    \"\"\"\n    Get Lemma using Prefix Decomposition Strategy\n\n    This method performs lemmatization by performing subword decomposition using pre-defined prefixes.\n    It checks if the language has known prefixes defined.\n    If a known prefix is found at the start of the token, it extracts the prefix and performs dictionary lookup on the remaining subword.\n    If a lemma is found for the subword, it returns the concatenation of the prefix and the lowercase subword.\n    If no known prefix is found or no lemma is found for the subword, None is returned.\n\n    Args:\n        token (str): The input token to lemmatize.\n        lang (str): The language code for the token's language.\n\n    Returns:\n        Optional[str]: The lemma for the token, or None if no lemma is found.\n\n    \"\"\"\n    if lang not in self._known_prefixes:\n        return None\n\n    prefix_match = self._known_prefixes[lang].match(token)\n    if not prefix_match or prefix_match[1] == token:\n        return None\n\n    prefix = prefix_match[1]\n\n    subword = self._dictionary_lookup.get_lemma(token[len(prefix) :], lang)\n\n    return prefix + subword.lower() if subword else None\n</code></pre>"},{"location":"reference/strategies/rules/","title":"Rules Strategy","text":"<p>This module defines the <code>RulesStrategy</code> class, which is a concrete implementation of the <code>LemmatizationStrategy</code> protocol. It provides lemmatization by applying pre-defined rules for each language.</p>"},{"location":"reference/strategies/rules/#simplemma.strategies.rules-classes","title":"Classes","text":""},{"location":"reference/strategies/rules/#simplemma.strategies.rules.RulesStrategy","title":"<code>RulesStrategy</code>","text":"<p>             Bases: <code>LemmatizationStrategy</code></p> <p>This class represents a lemmatization strategy that performs lemmatization by applying pre-defined rules for each language. It implements the <code>LemmatizationStrategy</code> protocol.</p> Source code in <code>simplemma/strategies/rules.py</code> <pre><code>class RulesStrategy(LemmatizationStrategy):\n    \"\"\"\n    This class represents a lemmatization strategy that performs lemmatization by applying pre-defined rules for each language.\n    It implements the `LemmatizationStrategy` protocol.\n    \"\"\"\n\n    __slots__ = [\"_rules\"]\n\n    def __init__(\n        self, rules: Dict[str, Callable[[str], Optional[str]]] = DEFAULT_RULES\n    ):\n        \"\"\"\n        Initialize the Rules Strategy.\n\n        Args:\n            rules (Dict[str, Callable[[str], Optional[str]]]): A dictionary of pre-defined rules for various languages.\n                Defaults to `DEFAULT_RULES`.\n\n        \"\"\"\n        self._rules = rules\n\n    def get_lemma(self, token: str, lang: str) -&gt; Optional[str]:\n        \"\"\"\n        Get Lemma using Rules Strategy\n\n        This method performs lemmatization by applying pre-defined rules for each language.\n        It checks if the language has pre-defined rules defined.\n        If rules are defined, it applies the corresponding rule on the token to get the lemma.\n        If a lemma is found, it is returned.\n        If no rules are defined for the language or no lemma is found, None is returned.\n\n        Args:\n            token (str): The input token to lemmatize.\n            lang (str): The language code for the token's language.\n\n        Returns:\n            Optional[str]: The lemma for the token, or None if no lemma is found.\n\n        \"\"\"\n        if lang not in self._rules:\n            return None\n\n        return self._rules[lang](token)\n</code></pre>"},{"location":"reference/strategies/rules/#simplemma.strategies.rules.RulesStrategy-functions","title":"Functions","text":""},{"location":"reference/strategies/rules/#simplemma.strategies.rules.RulesStrategy.__init__","title":"<code>__init__(rules=DEFAULT_RULES)</code>","text":"<p>Initialize the Rules Strategy.</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>Dict[str, Callable[[str], Optional[str]]]</code> <p>A dictionary of pre-defined rules for various languages. Defaults to <code>DEFAULT_RULES</code>.</p> <code>DEFAULT_RULES</code> Source code in <code>simplemma/strategies/rules.py</code> <pre><code>def __init__(\n    self, rules: Dict[str, Callable[[str], Optional[str]]] = DEFAULT_RULES\n):\n    \"\"\"\n    Initialize the Rules Strategy.\n\n    Args:\n        rules (Dict[str, Callable[[str], Optional[str]]]): A dictionary of pre-defined rules for various languages.\n            Defaults to `DEFAULT_RULES`.\n\n    \"\"\"\n    self._rules = rules\n</code></pre>"},{"location":"reference/strategies/rules/#simplemma.strategies.rules.RulesStrategy.get_lemma","title":"<code>get_lemma(token, lang)</code>","text":"<p>Get Lemma using Rules Strategy</p> <p>This method performs lemmatization by applying pre-defined rules for each language. It checks if the language has pre-defined rules defined. If rules are defined, it applies the corresponding rule on the token to get the lemma. If a lemma is found, it is returned. If no rules are defined for the language or no lemma is found, None is returned.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>str</code> <p>The input token to lemmatize.</p> required <code>lang</code> <code>str</code> <p>The language code for the token's language.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The lemma for the token, or None if no lemma is found.</p> Source code in <code>simplemma/strategies/rules.py</code> <pre><code>def get_lemma(self, token: str, lang: str) -&gt; Optional[str]:\n    \"\"\"\n    Get Lemma using Rules Strategy\n\n    This method performs lemmatization by applying pre-defined rules for each language.\n    It checks if the language has pre-defined rules defined.\n    If rules are defined, it applies the corresponding rule on the token to get the lemma.\n    If a lemma is found, it is returned.\n    If no rules are defined for the language or no lemma is found, None is returned.\n\n    Args:\n        token (str): The input token to lemmatize.\n        lang (str): The language code for the token's language.\n\n    Returns:\n        Optional[str]: The lemma for the token, or None if no lemma is found.\n\n    \"\"\"\n    if lang not in self._rules:\n        return None\n\n    return self._rules[lang](token)\n</code></pre>"},{"location":"reference/strategies/dictionaries/dictionary_factory/","title":"Dictionary Factory","text":"<p>This module defines the <code>DictionaryFactory</code> protocol and the <code>DefaultDictionaryFactory</code> class. It provides functionality for loading and accessing dictionaries for supported languages.</p> <ul> <li>DictionaryFactory: The Protocol class for all dictionary factories.</li> <li>DefaultDictionaryFactory: Default dictionary factory. It loads the dictionaries that are shipped with simplemma and caches them as configured.</li> </ul>"},{"location":"reference/strategies/dictionaries/dictionary_factory/#simplemma.strategies.dictionaries.dictionary_factory-classes","title":"Classes","text":""},{"location":"reference/strategies/dictionaries/dictionary_factory/#simplemma.strategies.dictionaries.dictionary_factory.DefaultDictionaryFactory","title":"<code>DefaultDictionaryFactory</code>","text":"<p>             Bases: <code>DictionaryFactory</code></p> <p>Default Dictionary Factory.</p> <p>This class is a concrete implementation of the <code>DictionaryFactory</code> protocol. It provides functionality for loading and caching dictionaries from disk that are included in Simplemma.</p> Source code in <code>simplemma/strategies/dictionaries/dictionary_factory.py</code> <pre><code>class DefaultDictionaryFactory(DictionaryFactory):\n    \"\"\"\n    Default Dictionary Factory.\n\n    This class is a concrete implementation of the `DictionaryFactory` protocol.\n    It provides functionality for loading and caching dictionaries from disk that are included in Simplemma.\n    \"\"\"\n\n    __slots__ = [\"_load_dictionary_from_disk\"]\n\n    def __init__(self, cache_max_size: int = 8) -&gt; None:\n        \"\"\"\n        Initialize the DefaultDictionaryFactory.\n\n        Args:\n            cache_max_size (int): The maximum size of the cache for loaded dictionaries.\n                Defaults to `8`.\n        \"\"\"\n        self._load_dictionary_from_disk = lru_cache(maxsize=cache_max_size)(\n            _load_dictionary_from_disk\n        )\n\n    def get_dictionary(\n        self,\n        lang: str,\n    ) -&gt; Mapping[str, str]:\n        \"\"\"\n        Get the dictionary for a specific language.\n\n        Args:\n            lang (str): The language code.\n\n        Returns:\n            Mapping[str, str]: The dictionary for the specified language.\n\n        Raises:\n            ValueError: If the specified language is not supported.\n        \"\"\"\n        if lang not in SUPPORTED_LANGUAGES:\n            raise ValueError(f\"Unsupported language: {lang}\")\n        return MappingStrToByteString(self._load_dictionary_from_disk(lang))\n</code></pre>"},{"location":"reference/strategies/dictionaries/dictionary_factory/#simplemma.strategies.dictionaries.dictionary_factory.DefaultDictionaryFactory-functions","title":"Functions","text":""},{"location":"reference/strategies/dictionaries/dictionary_factory/#simplemma.strategies.dictionaries.dictionary_factory.DefaultDictionaryFactory.__init__","title":"<code>__init__(cache_max_size=8)</code>","text":"<p>Initialize the DefaultDictionaryFactory.</p> <p>Parameters:</p> Name Type Description Default <code>cache_max_size</code> <code>int</code> <p>The maximum size of the cache for loaded dictionaries. Defaults to <code>8</code>.</p> <code>8</code> Source code in <code>simplemma/strategies/dictionaries/dictionary_factory.py</code> <pre><code>def __init__(self, cache_max_size: int = 8) -&gt; None:\n    \"\"\"\n    Initialize the DefaultDictionaryFactory.\n\n    Args:\n        cache_max_size (int): The maximum size of the cache for loaded dictionaries.\n            Defaults to `8`.\n    \"\"\"\n    self._load_dictionary_from_disk = lru_cache(maxsize=cache_max_size)(\n        _load_dictionary_from_disk\n    )\n</code></pre>"},{"location":"reference/strategies/dictionaries/dictionary_factory/#simplemma.strategies.dictionaries.dictionary_factory.DefaultDictionaryFactory.get_dictionary","title":"<code>get_dictionary(lang)</code>","text":"<p>Get the dictionary for a specific language.</p> <p>Parameters:</p> Name Type Description Default <code>lang</code> <code>str</code> <p>The language code.</p> required <p>Returns:</p> Type Description <code>Mapping[str, str]</code> <p>Mapping[str, str]: The dictionary for the specified language.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified language is not supported.</p> Source code in <code>simplemma/strategies/dictionaries/dictionary_factory.py</code> <pre><code>def get_dictionary(\n    self,\n    lang: str,\n) -&gt; Mapping[str, str]:\n    \"\"\"\n    Get the dictionary for a specific language.\n\n    Args:\n        lang (str): The language code.\n\n    Returns:\n        Mapping[str, str]: The dictionary for the specified language.\n\n    Raises:\n        ValueError: If the specified language is not supported.\n    \"\"\"\n    if lang not in SUPPORTED_LANGUAGES:\n        raise ValueError(f\"Unsupported language: {lang}\")\n    return MappingStrToByteString(self._load_dictionary_from_disk(lang))\n</code></pre>"},{"location":"reference/strategies/dictionaries/dictionary_factory/#simplemma.strategies.dictionaries.dictionary_factory.DictionaryFactory","title":"<code>DictionaryFactory</code>","text":"<p>             Bases: <code>Protocol</code></p> <p>This protocol defines the interface for a dictionary factory, which is responsible for loading and providing access to dictionaries for different languages.</p> Note <p>This protocol should be implemented by concrete dictionary factories. Concrete implementations of this protocol should provide a concrete implementation for the <code>get_dictionary</code> method.</p> Source code in <code>simplemma/strategies/dictionaries/dictionary_factory.py</code> <pre><code>class DictionaryFactory(Protocol):\n    \"\"\"\n    This protocol defines the interface for a dictionary factory, which is responsible for loading and providing access to dictionaries for different languages.\n\n    Note:\n        This protocol should be implemented by concrete dictionary factories.\n        Concrete implementations of this protocol should provide a concrete implementation for the `get_dictionary` method.\n    \"\"\"\n\n    @abstractmethod\n    def get_dictionary(\n        self,\n        lang: str,\n    ) -&gt; Mapping[str, str]:\n        \"\"\"\n        Get the dictionary for a specific language.\n\n        Args:\n            lang (str): The language code.\n\n        Returns:\n            Mapping[str, str]: The dictionary for the specified language.\n\n        Raises:\n            ValueError: If the specified language is not supported.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/strategies/dictionaries/dictionary_factory/#simplemma.strategies.dictionaries.dictionary_factory.DictionaryFactory-functions","title":"Functions","text":""},{"location":"reference/strategies/dictionaries/dictionary_factory/#simplemma.strategies.dictionaries.dictionary_factory.DictionaryFactory.get_dictionary","title":"<code>get_dictionary(lang)</code>  <code>abstractmethod</code>","text":"<p>Get the dictionary for a specific language.</p> <p>Parameters:</p> Name Type Description Default <code>lang</code> <code>str</code> <p>The language code.</p> required <p>Returns:</p> Type Description <code>Mapping[str, str]</code> <p>Mapping[str, str]: The dictionary for the specified language.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified language is not supported.</p> Source code in <code>simplemma/strategies/dictionaries/dictionary_factory.py</code> <pre><code>@abstractmethod\ndef get_dictionary(\n    self,\n    lang: str,\n) -&gt; Mapping[str, str]:\n    \"\"\"\n    Get the dictionary for a specific language.\n\n    Args:\n        lang (str): The language code.\n\n    Returns:\n        Mapping[str, str]: The dictionary for the specified language.\n\n    Raises:\n        ValueError: If the specified language is not supported.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/strategies/dictionaries/dictionary_factory/#simplemma.strategies.dictionaries.dictionary_factory.MappingStrToByteString","title":"<code>MappingStrToByteString</code>","text":"<p>             Bases: <code>Mapping[str, str]</code></p> <p>Wrapper around ByString dict to make them behave like str dict.</p> Source code in <code>simplemma/strategies/dictionaries/dictionary_factory.py</code> <pre><code>class MappingStrToByteString(Mapping[str, str]):\n    \"\"\"Wrapper around ByString dict to make them behave like str dict.\"\"\"\n\n    __slots__ = [\"_dict\"]\n\n    def __init__(self, dictionary: Dict[bytes, bytes]) -&gt; None:\n        self._dict = dictionary\n\n    def __getitem__(self, item: str) -&gt; str:\n        return self._dict[item.encode()].decode()\n\n    def __iter__(self) -&gt; Iterator[str]:\n        for key in self._dict:\n            yield key.decode()\n\n    def __len__(self) -&gt; int:\n        return len(self._dict)\n</code></pre>"},{"location":"reference/strategies/fallback/lemmatization_fallback_strategy/","title":"Lemmatization Fallback Strategy","text":"<p>This module defines the <code>LemmatizationFallbackStrategy</code> protocol, which represents the interface for lemmatization fallback strategies in the Simplemma library. <code>LemmatizationFallbackStrategy</code> are used as a fallback strategy when a token's lemma cannot be determined using other lemmatization strategies.</p>"},{"location":"reference/strategies/fallback/lemmatization_fallback_strategy/#simplemma.strategies.fallback.lemmatization_fallback_strategy-classes","title":"Classes","text":""},{"location":"reference/strategies/fallback/lemmatization_fallback_strategy/#simplemma.strategies.fallback.lemmatization_fallback_strategy.LemmatizationFallbackStrategy","title":"<code>LemmatizationFallbackStrategy</code>","text":"<p>             Bases: <code>Protocol</code></p> <p>This protocol defines the interface for lemmatization fallback strategies in the Simplemma library. Fallback strategies are used when a token's lemma cannot be determined using other lemmatization strategies.</p> <p>Note:      This protocol should be implemented by concrete fallback strategy classes.      Concrete implementations of this protocol should provide a concrete implementation for the <code>get_lemma</code> method.</p> Source code in <code>simplemma/strategies/fallback/lemmatization_fallback_strategy.py</code> <pre><code>class LemmatizationFallbackStrategy(Protocol):\n    \"\"\"\n    This protocol defines the interface for lemmatization fallback strategies in the Simplemma library.\n    Fallback strategies are used when a token's lemma cannot be determined using other lemmatization strategies.\n\n     Note:\n         This protocol should be implemented by concrete fallback strategy classes.\n         Concrete implementations of this protocol should provide a concrete implementation for the `get_lemma` method.\n    \"\"\"\n\n    @abstractmethod\n    def get_lemma(self, token: str, lang: str) -&gt; str:\n        \"\"\"\n        Retrieve the lemma of a given token in the specified language.\n\n        This method takes a token and a language and returns the lemma of the token in the specified language.\n\n        Args:\n            token (str): The token for which to retrieve the lemma.\n            lang (str): The language of the token.\n\n        Returns:\n            str: The lemma of the token in the specified language.\n\n        Raises:\n            NotImplementedError: This method must be implemented by concrete classes.\n        \"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference/strategies/fallback/lemmatization_fallback_strategy/#simplemma.strategies.fallback.lemmatization_fallback_strategy.LemmatizationFallbackStrategy-functions","title":"Functions","text":""},{"location":"reference/strategies/fallback/lemmatization_fallback_strategy/#simplemma.strategies.fallback.lemmatization_fallback_strategy.LemmatizationFallbackStrategy.get_lemma","title":"<code>get_lemma(token, lang)</code>  <code>abstractmethod</code>","text":"<p>Retrieve the lemma of a given token in the specified language.</p> <p>This method takes a token and a language and returns the lemma of the token in the specified language.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>str</code> <p>The token for which to retrieve the lemma.</p> required <code>lang</code> <code>str</code> <p>The language of the token.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The lemma of the token in the specified language.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by concrete classes.</p> Source code in <code>simplemma/strategies/fallback/lemmatization_fallback_strategy.py</code> <pre><code>@abstractmethod\ndef get_lemma(self, token: str, lang: str) -&gt; str:\n    \"\"\"\n    Retrieve the lemma of a given token in the specified language.\n\n    This method takes a token and a language and returns the lemma of the token in the specified language.\n\n    Args:\n        token (str): The token for which to retrieve the lemma.\n        lang (str): The language of the token.\n\n    Returns:\n        str: The lemma of the token in the specified language.\n\n    Raises:\n        NotImplementedError: This method must be implemented by concrete classes.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/strategies/fallback/raise_error/","title":"Raise Error Strategy","text":"<p>This module defines the <code>RaiseErrorFallbackStrategy</code> class, which is a concrete implementation of the <code>LemmatizationFallbackStrategy</code> protocol. It represents a fallback strategy that raises a <code>ValueError</code> when the lemma of a token cannot be determined.</p>"},{"location":"reference/strategies/fallback/raise_error/#simplemma.strategies.fallback.raise_error-classes","title":"Classes","text":""},{"location":"reference/strategies/fallback/raise_error/#simplemma.strategies.fallback.raise_error.RaiseErrorFallbackStrategy","title":"<code>RaiseErrorFallbackStrategy</code>","text":"<p>             Bases: <code>LemmatizationFallbackStrategy</code></p> <p>Raise Error Fallback Strategy. RaiseErrorFallbackStrategy is a concrete implementation of the LemmatizationFallbackStrategy protocol. It represents a fallback strategy that raises a ValueError when the lemma of a token cannot be determined.</p> Source code in <code>simplemma/strategies/fallback/raise_error.py</code> <pre><code>class RaiseErrorFallbackStrategy(LemmatizationFallbackStrategy):\n    \"\"\"\n    Raise Error Fallback Strategy.\n    RaiseErrorFallbackStrategy is a concrete implementation of the LemmatizationFallbackStrategy protocol. It represents\n    a fallback strategy that raises a ValueError when the lemma of a token cannot be determined.\n    \"\"\"\n\n    def get_lemma(self, token: str, lang: str) -&gt; str:\n        \"\"\"\n        Raise a ValueError indicating that the token was not found.\n\n        This method is called when the lemma of a token cannot be determined using other lemmatization strategies.\n        It raises a ValueError with an appropriate error message indicating that the token was not found.\n\n        Args:\n            token (str): The token for which the lemma could not be determined.\n            lang (str): The language of the token.\n\n        Raises:\n            ValueError: The token was not found and its lemma cannot be determined.\n        \"\"\"\n        raise ValueError(f\"Token not found: {token}\")\n</code></pre>"},{"location":"reference/strategies/fallback/raise_error/#simplemma.strategies.fallback.raise_error.RaiseErrorFallbackStrategy-functions","title":"Functions","text":""},{"location":"reference/strategies/fallback/raise_error/#simplemma.strategies.fallback.raise_error.RaiseErrorFallbackStrategy.get_lemma","title":"<code>get_lemma(token, lang)</code>","text":"<p>Raise a ValueError indicating that the token was not found.</p> <p>This method is called when the lemma of a token cannot be determined using other lemmatization strategies. It raises a ValueError with an appropriate error message indicating that the token was not found.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>str</code> <p>The token for which the lemma could not be determined.</p> required <code>lang</code> <code>str</code> <p>The language of the token.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>The token was not found and its lemma cannot be determined.</p> Source code in <code>simplemma/strategies/fallback/raise_error.py</code> <pre><code>def get_lemma(self, token: str, lang: str) -&gt; str:\n    \"\"\"\n    Raise a ValueError indicating that the token was not found.\n\n    This method is called when the lemma of a token cannot be determined using other lemmatization strategies.\n    It raises a ValueError with an appropriate error message indicating that the token was not found.\n\n    Args:\n        token (str): The token for which the lemma could not be determined.\n        lang (str): The language of the token.\n\n    Raises:\n        ValueError: The token was not found and its lemma cannot be determined.\n    \"\"\"\n    raise ValueError(f\"Token not found: {token}\")\n</code></pre>"},{"location":"reference/strategies/fallback/to_lowercase/","title":"To Lowercase Strategy","text":"<p>This module defines the <code>ToLowercaseFallbackStrategy</code> class, which is a concrete implementation of the <code>LemmatizationFallbackStrategy</code> protocol. It represents a fallback strategy that converts tokens to lowercase for specific languages.</p>"},{"location":"reference/strategies/fallback/to_lowercase/#simplemma.strategies.fallback.to_lowercase-classes","title":"Classes","text":""},{"location":"reference/strategies/fallback/to_lowercase/#simplemma.strategies.fallback.to_lowercase.ToLowercaseFallbackStrategy","title":"<code>ToLowercaseFallbackStrategy</code>","text":"<p>             Bases: <code>LemmatizationFallbackStrategy</code></p> <p>ToLowercaseFallbackStrategy is a concrete implementation of the LemmatizationFallbackStrategy protocol. It represents a fallback strategy that converts tokens to lowercase for specific languages.</p> Source code in <code>simplemma/strategies/fallback/to_lowercase.py</code> <pre><code>class ToLowercaseFallbackStrategy(LemmatizationFallbackStrategy):\n    \"\"\"\n    ToLowercaseFallbackStrategy is a concrete implementation of the LemmatizationFallbackStrategy protocol.\n    It represents a fallback strategy that converts tokens to lowercase for specific languages.\n    \"\"\"\n\n    __slots__ = [\"_langs_to_lower\"]\n\n    def __init__(self, langs_to_lower: Set[str] = BETTER_LOWER):\n        \"\"\"\n        Initialize the ToLowercaseFallbackStrategy with the specified set of languages to convert to lowercase.\n\n        Args:\n            langs_to_lower (Set[str]): The set of languages for which tokens should be converted to lowercase.\n                Defaults to `BETTER_LOWER`.\n\n        \"\"\"\n        self._langs_to_lower = langs_to_lower\n\n    def get_lemma(self, token: str, lang: str) -&gt; str:\n        \"\"\"\n        Convert the token to lowercase if the language is in the set of languages to convert.\n\n        This method is called when the lemma of a token cannot be determined using other lemmatization strategies.\n        It converts the token to lowercase if the language is in the set of languages specified during initialization.\n\n        Args:\n            token (str): The token for which the lemma could not be determined.\n            lang (str): The language of the token.\n\n        Returns:\n            str: The lowercase version of the token if the language is in the set of languages to convert,\n                 otherwise returns the original token.\n\n        \"\"\"\n        return token.lower() if lang in self._langs_to_lower else token\n</code></pre>"},{"location":"reference/strategies/fallback/to_lowercase/#simplemma.strategies.fallback.to_lowercase.ToLowercaseFallbackStrategy-functions","title":"Functions","text":""},{"location":"reference/strategies/fallback/to_lowercase/#simplemma.strategies.fallback.to_lowercase.ToLowercaseFallbackStrategy.__init__","title":"<code>__init__(langs_to_lower=BETTER_LOWER)</code>","text":"<p>Initialize the ToLowercaseFallbackStrategy with the specified set of languages to convert to lowercase.</p> <p>Parameters:</p> Name Type Description Default <code>langs_to_lower</code> <code>Set[str]</code> <p>The set of languages for which tokens should be converted to lowercase. Defaults to <code>BETTER_LOWER</code>.</p> <code>BETTER_LOWER</code> Source code in <code>simplemma/strategies/fallback/to_lowercase.py</code> <pre><code>def __init__(self, langs_to_lower: Set[str] = BETTER_LOWER):\n    \"\"\"\n    Initialize the ToLowercaseFallbackStrategy with the specified set of languages to convert to lowercase.\n\n    Args:\n        langs_to_lower (Set[str]): The set of languages for which tokens should be converted to lowercase.\n            Defaults to `BETTER_LOWER`.\n\n    \"\"\"\n    self._langs_to_lower = langs_to_lower\n</code></pre>"},{"location":"reference/strategies/fallback/to_lowercase/#simplemma.strategies.fallback.to_lowercase.ToLowercaseFallbackStrategy.get_lemma","title":"<code>get_lemma(token, lang)</code>","text":"<p>Convert the token to lowercase if the language is in the set of languages to convert.</p> <p>This method is called when the lemma of a token cannot be determined using other lemmatization strategies. It converts the token to lowercase if the language is in the set of languages specified during initialization.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>str</code> <p>The token for which the lemma could not be determined.</p> required <code>lang</code> <code>str</code> <p>The language of the token.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The lowercase version of the token if the language is in the set of languages to convert,  otherwise returns the original token.</p> Source code in <code>simplemma/strategies/fallback/to_lowercase.py</code> <pre><code>def get_lemma(self, token: str, lang: str) -&gt; str:\n    \"\"\"\n    Convert the token to lowercase if the language is in the set of languages to convert.\n\n    This method is called when the lemma of a token cannot be determined using other lemmatization strategies.\n    It converts the token to lowercase if the language is in the set of languages specified during initialization.\n\n    Args:\n        token (str): The token for which the lemma could not be determined.\n        lang (str): The language of the token.\n\n    Returns:\n        str: The lowercase version of the token if the language is in the set of languages to convert,\n             otherwise returns the original token.\n\n    \"\"\"\n    return token.lower() if lang in self._langs_to_lower else token\n</code></pre>"}]}